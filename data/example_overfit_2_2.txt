Batch 0 loss is 0.5532917976379395
At batch 0 layer 0 has weights [-0.31950950622558594, 0.294607937335968] and bias 0.48033076524734497
Batch 100 loss is 0.4763953685760498
At batch 100 layer 0 has weights [-0.22102470695972443, 0.3901585042476654] and bias 0.46407178044319153
Batch 200 loss is 0.3558175265789032
At batch 200 layer 0 has weights [-0.12050314247608185, 0.4884415864944458] and bias 0.4641777575016022
Batch 300 loss is 0.23125313222408295
At batch 300 layer 0 has weights [-0.031437601894140244, 0.5765566229820251] and bias 0.4547163248062134
Batch 400 loss is 0.21976032853126526
At batch 400 layer 0 has weights [0.017098788172006607, 0.6117799282073975] and bias 0.3707980513572693
Batch 500 loss is 0.2199563980102539
At batch 500 layer 0 has weights [0.05974370613694191, 0.638795018196106] and bias 0.30014991760253906
Batch 600 loss is 0.21470974385738373
At batch 600 layer 0 has weights [0.10231756418943405, 0.6640567779541016] and bias 0.233913853764534
Batch 700 loss is 0.11747278273105621
At batch 700 layer 0 has weights [0.1424144208431244, 0.6873067021369934] and bias 0.170126274228096
Batch 800 loss is 0.08901242166757584
At batch 800 layer 0 has weights [0.18212415277957916, 0.7076813578605652] and bias 0.11011018604040146
Batch 900 loss is 0.06556712836027145
At batch 900 layer 0 has weights [0.22116722166538239, 0.7297728657722473] and bias 0.048922739923000336
Batch 1000 loss is 0.0005343263037502766
At batch 1000 layer 0 has weights [0.25581198930740356, 0.744737446308136] and bias 0.00025957924663089216
Batch 1100 loss is 0.04622982442378998
At batch 1100 layer 0 has weights [0.2648126184940338, 0.7352859973907471] and bias 0.00022512725263368338
Batch 1200 loss is 0.022996768355369568
At batch 1200 layer 0 has weights [0.2741813659667969, 0.726051926612854] and bias -0.0004525773401837796
Batch 1300 loss is 0.08974142372608185
At batch 1300 layer 0 has weights [0.28415024280548096, 0.716064989566803] and bias 0.0001390892284689471
Batch 1400 loss is 0.15465229749679565
At batch 1400 layer 0 has weights [0.2933851182460785, 0.70673668384552] and bias -0.0001833490387070924
Batch 1500 loss is 0.17483384907245636
At batch 1500 layer 0 has weights [0.30278414487838745, 0.6978200078010559] and bias 3.9014605135889724e-05
Batch 1600 loss is 0.0005838422803208232
At batch 1600 layer 0 has weights [0.31173670291900635, 0.6882991194725037] and bias 0.0005581374280154705
Batch 1700 loss is 0.06398989260196686
At batch 1700 layer 0 has weights [0.3215775489807129, 0.678790271282196] and bias 0.0001233940274687484
Batch 1800 loss is 0.06282567977905273
At batch 1800 layer 0 has weights [0.33122873306274414, 0.6688361763954163] and bias -0.00019791150407399982
Batch 1900 loss is 0.04151299595832825
At batch 1900 layer 0 has weights [0.3410767614841461, 0.65947425365448] and bias -0.00010013065912062302
Batch 2000 loss is 0.06128066033124924
At batch 2000 layer 0 has weights [0.3498038947582245, 0.6496181488037109] and bias -0.00024411834601778537
Batch 2100 loss is 0.08027638494968414
At batch 2100 layer 0 has weights [0.35938113927841187, 0.6409450769424438] and bias 2.5753302907105535e-05
Batch 2200 loss is 0.019703136757016182
At batch 2200 layer 0 has weights [0.37003594636917114, 0.6299322843551636] and bias 3.463385655777529e-05
Batch 2300 loss is 0.07774564623832703
At batch 2300 layer 0 has weights [0.37889689207077026, 0.6213035583496094] and bias -0.0002105234598275274
Batch 2400 loss is 0.07643851637840271
At batch 2400 layer 0 has weights [0.38874351978302, 0.6112416982650757] and bias 4.206087760394439e-05
Batch 2500 loss is 0.03764158487319946
At batch 2500 layer 0 has weights [0.39856693148612976, 0.6015264987945557] and bias -4.412238013173919e-06
Batch 2600 loss is 0.037382904440164566
At batch 2600 layer 0 has weights [0.40877270698547363, 0.5909863114356995] and bias -0.0003464959445409477
Batch 2700 loss is 0.018217677250504494
At batch 2700 layer 0 has weights [0.41905656456947327, 0.5808085799217224] and bias 3.533457129378803e-05
Batch 2800 loss is 0.03604342043399811
At batch 2800 layer 0 has weights [0.42835503816604614, 0.5709614157676697] and bias 7.690196071052924e-05
Batch 2900 loss is 0.10553628951311111
At batch 2900 layer 0 has weights [0.4381546974182129, 0.562149167060852] and bias -6.426727486541495e-05
Batch 3000 loss is 0.10445364564657211
At batch 3000 layer 0 has weights [0.44792553782463074, 0.5528795123100281] and bias 0.00046171151916496456
Batch 3100 loss is 0.0172312930226326
At batch 3100 layer 0 has weights [0.4582190215587616, 0.5423545241355896] and bias -6.446008774219081e-05
Batch 3200 loss is 0.0834900438785553
At batch 3200 layer 0 has weights [0.46754661202430725, 0.5320727825164795] and bias -0.0002202965843025595
Batch 3300 loss is 0.032827917486429214
At batch 3300 layer 0 has weights [0.47750362753868103, 0.5221214294433594] and bias 0.00031329732155427337
Batch 3400 loss is 0.016988558694720268
At batch 3400 layer 0 has weights [0.48628491163253784, 0.5129917860031128] and bias -0.0007079628412611783
Batch 3500 loss is 0.06331945210695267
At batch 3500 layer 0 has weights [0.496033251285553, 0.5036306977272034] and bias -0.0002914504148066044
Batch 3600 loss is 0.015546164475381374
At batch 3600 layer 0 has weights [0.5063135027885437, 0.4934267997741699] and bias 7.743600144749507e-05
Batch 3700 loss is 0.09104850143194199
At batch 3700 layer 0 has weights [0.5158042311668396, 0.4844224452972412] and bias 0.00011299762991257012
Batch 3800 loss is 0.044723719358444214
At batch 3800 layer 0 has weights [0.5248950123786926, 0.4750228524208069] and bias -0.00018370759789831936
Batch 3900 loss is 0.11639055609703064
At batch 3900 layer 0 has weights [0.5347020626068115, 0.46520355343818665] and bias 0.0001397943851770833
Batch 4000 loss is 0.028651872649788857
At batch 4000 layer 0 has weights [0.5452192425727844, 0.45478013157844543] and bias 0.0002285204391228035
Batch 4100 loss is 0.04189690202474594
At batch 4100 layer 0 has weights [0.5549511313438416, 0.44529569149017334] and bias 1.9367005734238774e-05
Batch 4200 loss is 0.04091198742389679
At batch 4200 layer 0 has weights [0.5652406811714172, 0.4349549114704132] and bias -0.0003015508991666138
Batch 4300 loss is 0.0799197182059288
At batch 4300 layer 0 has weights [0.5747090578079224, 0.4246649146080017] and bias 2.5347169867018238e-05
Batch 4400 loss is 0.02625724859535694
At batch 4400 layer 0 has weights [0.5852053761482239, 0.41529470682144165] and bias 5.1313912990735844e-05
Batch 4500 loss is 0.038141004741191864
At batch 4500 layer 0 has weights [0.5951815247535706, 0.404567152261734] and bias -8.79849394550547e-05
Batch 4600 loss is 0.012591706588864326
At batch 4600 layer 0 has weights [0.6051586866378784, 0.394690603017807] and bias -0.00020950192993041128
Batch 4700 loss is 0.00019889581017196178
At batch 4700 layer 0 has weights [0.6148974299430847, 0.38497141003608704] and bias -0.00011693126725731418
Batch 4800 loss is 0.03524642437696457
At batch 4800 layer 0 has weights [0.6251947283744812, 0.3750006854534149] and bias -4.040166095364839e-05
Batch 4900 loss is 0.05718526616692543
At batch 4900 layer 0 has weights [0.6348180770874023, 0.3650670349597931] and bias -0.00011999662092421204
Batch 5000 loss is 0.022279346361756325
At batch 5000 layer 0 has weights [0.6452710628509521, 0.35467174649238586] and bias -4.258587068761699e-05
