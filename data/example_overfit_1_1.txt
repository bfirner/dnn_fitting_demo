Batch 0 loss is 0.8887677192687988
At batch 0 layer 0 has weights [-0.5359944105148315, 0.3965551257133484] and bias -0.24626362323760986
Batch 100 loss is 0.516569972038269
At batch 100 layer 0 has weights [-0.43947112560272217, 0.4930765926837921] and bias -0.14626342058181763
Batch 200 loss is 0.401669979095459
At batch 200 layer 0 has weights [-0.34133753180503845, 0.5903794765472412] and bias -0.046263281255960464
Batch 300 loss is 0.3432805836200714
At batch 300 layer 0 has weights [-0.24482889473438263, 0.6870987415313721] and bias 0.008943779394030571
Batch 400 loss is 0.19889047741889954
At batch 400 layer 0 has weights [-0.14417356252670288, 0.7878900170326233] and bias 0.012152149342000484
Batch 500 loss is 0.22602134943008423
At batch 500 layer 0 has weights [-0.04534338414669037, 0.8868434429168701] and bias 0.013633382506668568
Batch 600 loss is 0.0009456173283979297
At batch 600 layer 0 has weights [0.03524162247776985, 0.9646685123443604] and bias 0.0009877346456050873
Batch 700 loss is 0.030066996812820435
At batch 700 layer 0 has weights [0.040147412568330765, 0.9600271582603455] and bias -3.7176690966589376e-05
Batch 800 loss is 0.059797950088977814
At batch 800 layer 0 has weights [0.044279273599386215, 0.9555440545082092] and bias 0.00015167411766014993
Batch 900 loss is 0.05964963883161545
At batch 900 layer 0 has weights [0.04949099197983742, 0.9506689310073853] and bias 0.00018042116425931454
Batch 1000 loss is 0.14795894920825958
At batch 1000 layer 0 has weights [0.05407825857400894, 0.9458888173103333] and bias -0.00014530320186167955
Batch 1100 loss is 0.00014524861762765795
At batch 1100 layer 0 has weights [0.05991927534341812, 0.939816415309906] and bias 0.00023707999207545072
Batch 1200 loss is 0.08818843960762024
At batch 1200 layer 0 has weights [0.06492019444704056, 0.9351150393486023] and bias -0.0005379430949687958
Batch 1300 loss is 0.000707614584825933
At batch 1300 layer 0 has weights [0.06998351961374283, 0.9291819334030151] and bias -0.00026425792020745575
Batch 1400 loss is 0.08739381283521652
At batch 1400 layer 0 has weights [0.07562988251447678, 0.9254545569419861] and bias 0.00036128272768110037
Batch 1500 loss is 0.08628065139055252
At batch 1500 layer 0 has weights [0.08039072901010513, 0.9195185303688049] and bias 0.00013863708591088653
Batch 1600 loss is 0.028873052448034286
At batch 1600 layer 0 has weights [0.08589447289705276, 0.9139283299446106] and bias -0.00023526838049292564
Batch 1700 loss is 0.08538098633289337
At batch 1700 layer 0 has weights [0.0912671834230423, 0.9088448286056519] and bias 0.0001874144363682717
Batch 1800 loss is 0.028633924201130867
At batch 1800 layer 0 has weights [0.0965607687830925, 0.9026913046836853] and bias 0.00010270092025166377
Batch 1900 loss is 0.0565599650144577
At batch 1900 layer 0 has weights [0.10283350199460983, 0.8970775008201599] and bias -0.000442566757556051
Batch 2000 loss is 0.05605047196149826
At batch 2000 layer 0 has weights [0.10796434432268143, 0.8916351199150085] and bias -0.0001480287464801222
Batch 2100 loss is 0.05565375089645386
At batch 2100 layer 0 has weights [0.1127123236656189, 0.8869197368621826] and bias -3.728625233634375e-05
Batch 2200 loss is 0.027699079364538193
At batch 2200 layer 0 has weights [0.11831790953874588, 0.8813053965568542] and bias -5.257427801552694e-06
Batch 2300 loss is 0.08228360861539841
At batch 2300 layer 0 has weights [0.12341132760047913, 0.8762957453727722] and bias 2.19067314901622e-05
Batch 2400 loss is 0.05450380966067314
At batch 2400 layer 0 has weights [0.12864840030670166, 0.8712378740310669] and bias 2.7954118195339106e-05
Batch 2500 loss is 0.027222227305173874
At batch 2500 layer 0 has weights [0.1338040828704834, 0.8660573363304138] and bias -0.00010164656123379245
Batch 2600 loss is 0.0003033233224414289
At batch 2600 layer 0 has weights [0.13916121423244476, 0.8604577779769897] and bias -0.00011282214836683124
Batch 2700 loss is 0.08014671504497528
At batch 2700 layer 0 has weights [0.14539481699466705, 0.8545340299606323] and bias 1.2634743143280502e-05
Batch 2800 loss is 5.475072975968942e-05
At batch 2800 layer 0 has weights [0.15120865404605865, 0.8487629890441895] and bias -4.411361805978231e-05
Batch 2900 loss is 0.026551945134997368
At batch 2900 layer 0 has weights [0.15677410364151, 0.8431894779205322] and bias 0.0002242446644231677
Batch 3000 loss is 0.0004240153939463198
At batch 3000 layer 0 has weights [0.1628401279449463, 0.8374625444412231] and bias 0.0002632281684782356
Batch 3100 loss is 0.00038837952888570726
At batch 3100 layer 0 has weights [0.16843125224113464, 0.8317877650260925] and bias 0.00028571547591127455
Batch 3200 loss is 0.07829661667346954
At batch 3200 layer 0 has weights [0.17255885899066925, 0.8268967866897583] and bias -0.00043480232125148177
Batch 3300 loss is 7.081752846715972e-05
At batch 3300 layer 0 has weights [0.1777048110961914, 0.8221456408500671] and bias 0.00013831007527187467
Batch 3400 loss is 0.02601415477693081
At batch 3400 layer 0 has weights [0.18208396434783936, 0.8173058032989502] and bias -5.3795662097400054e-05
Batch 3500 loss is 5.596542177954689e-05
At batch 3500 layer 0 has weights [0.18739928305149078, 0.8127255439758301] and bias -2.8027377993566915e-05
Batch 3600 loss is 0.025355031713843346
At batch 3600 layer 0 has weights [0.19252046942710876, 0.8073589205741882] and bias -9.491429227637127e-05
Batch 3700 loss is 0.025136495009064674
At batch 3700 layer 0 has weights [0.19819773733615875, 0.8018881678581238] and bias 5.687224620487541e-05
Batch 3800 loss is 0.0002003825211431831
At batch 3800 layer 0 has weights [0.20425181090831757, 0.7963745594024658] and bias -1.2407364920363761e-05
Batch 3900 loss is 0.04955478757619858
At batch 3900 layer 0 has weights [0.2106793373823166, 0.7898197174072266] and bias -0.00016232002235483378
Batch 4000 loss is 0.07377668470144272
At batch 4000 layer 0 has weights [0.215977281332016, 0.7843591570854187] and bias 0.00010503568773856387
Batch 4100 loss is 0.0487939715385437
At batch 4100 layer 0 has weights [0.22206425666809082, 0.7782817482948303] and bias -0.00021794492204207927
Batch 4200 loss is 0.0003934255801141262
At batch 4200 layer 0 has weights [0.22838398814201355, 0.772236168384552] and bias 4.4629996409639716e-05
Batch 4300 loss is 5.1356342737562954e-05
At batch 4300 layer 0 has weights [0.23338156938552856, 0.766574501991272] and bias -2.527570177335292e-05
Batch 4400 loss is 0.023840177804231644
At batch 4400 layer 0 has weights [0.2394660860300064, 0.7603755593299866] and bias 7.721949805272743e-05
Batch 4500 loss is 0.14163152873516083
At batch 4500 layer 0 has weights [0.2452617883682251, 0.7549737691879272] and bias -0.00011955242371186614
Batch 4600 loss is 0.023725342005491257
At batch 4600 layer 0 has weights [0.2507845461368561, 0.7485572695732117] and bias 0.0003907020727638155
Batch 4700 loss is 0.04677116870880127
At batch 4700 layer 0 has weights [0.25642719864845276, 0.7440066337585449] and bias 0.00015451700892299414
Batch 4800 loss is 0.04627097398042679
At batch 4800 layer 0 has weights [0.26174911856651306, 0.7384265065193176] and bias 5.4846354032633826e-05
Batch 4900 loss is 0.023036736994981766
At batch 4900 layer 0 has weights [0.26694920659065247, 0.7329182624816895] and bias -7.505660323658958e-05
Batch 5000 loss is 0.0002420034579699859
At batch 5000 layer 0 has weights [0.2723434567451477, 0.7273514866828918] and bias 0.00014185997133608907
