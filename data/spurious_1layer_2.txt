Batch 0 loss is 1.3219250440597534
At batch 0 layer 0 has weights [-0.27286264300346375, -0.5404255390167236] and bias -0.5792763829231262
Batch 100 loss is 1.2712417840957642
At batch 100 layer 0 has weights [-0.17167846858501434, -0.4397329092025757] and bias -0.47927767038345337
Batch 200 loss is 1.0981652736663818
At batch 200 layer 0 has weights [-0.07287246733903885, -0.34039512276649475] and bias -0.3792789578437805
Batch 300 loss is 0.9906715750694275
At batch 300 layer 0 has weights [0.028212571516633034, -0.2389201819896698] and bias -0.27928024530410767
Batch 400 loss is 0.6102443933486938
At batch 400 layer 0 has weights [0.12594185769557953, -0.1412755399942398] and bias -0.17928047478199005
Batch 500 loss is 0.6375876665115356
At batch 500 layer 0 has weights [0.22285869717597961, -0.04270711913704872] and bias -0.07928027212619781
Batch 600 loss is 0.37192440032958984
At batch 600 layer 0 has weights [0.32068726420402527, 0.05615701526403427] and bias 0.008448661305010319
Batch 700 loss is 0.29576194286346436
At batch 700 layer 0 has weights [0.4170983135700226, 0.15269726514816284] and bias 0.0069862352684140205
Batch 800 loss is 0.16154971718788147
At batch 800 layer 0 has weights [0.5168830156326294, 0.25226038694381714] and bias 0.008136811666190624
Batch 900 loss is 0.07206026464700699
At batch 900 layer 0 has weights [0.6139585971832275, 0.3496514558792114] and bias 0.005910990294069052
Batch 1000 loss is 0.045053426176309586
At batch 1000 layer 0 has weights [0.6400033831596375, 0.3598281145095825] and bias 0.0001589624589541927
Batch 1100 loss is 0.011275194585323334
At batch 1100 layer 0 has weights [0.6514855623245239, 0.3486950397491455] and bias -0.00046313615166582167
Batch 1200 loss is 0.03179485350847244
At batch 1200 layer 0 has weights [0.662651777267456, 0.33722567558288574] and bias -0.00010334552644053474
Batch 1300 loss is 0.030723612755537033
At batch 1300 layer 0 has weights [0.6747593283653259, 0.32552003860473633] and bias 0.00017842712986748666
Batch 1400 loss is 0.03948189318180084
At batch 1400 layer 0 has weights [0.6849411725997925, 0.31519031524658203] and bias -0.00016529740241821855
Batch 1500 loss is 0.0476042665541172
At batch 1500 layer 0 has weights [0.6967133283615112, 0.3037445545196533] and bias -0.0001232446520589292
Batch 1600 loss is 0.03658716380596161
At batch 1600 layer 0 has weights [0.7093793153762817, 0.29108089208602905] and bias -0.0002890828181989491
Batch 1700 loss is 0.02646070346236229
At batch 1700 layer 0 has weights [0.7202770709991455, 0.27902042865753174] and bias 9.148030949290842e-05
Batch 1800 loss is 0.008536373265087605
At batch 1800 layer 0 has weights [0.7324517369270325, 0.26741090416908264] and bias -0.00012829143088310957
Batch 1900 loss is 0.0482497364282608
At batch 1900 layer 0 has weights [0.7436996698379517, 0.2563127875328064] and bias -0.0001988666772376746
Batch 2000 loss is 0.023023704066872597
At batch 2000 layer 0 has weights [0.7557497620582581, 0.2444237619638443] and bias 7.40417162887752e-05
Batch 2100 loss is 0.01478560734540224
At batch 2100 layer 0 has weights [0.7666996121406555, 0.23337224125862122] and bias 0.00018984684720635414
Batch 2200 loss is 0.0280715674161911
At batch 2200 layer 0 has weights [0.7784979939460754, 0.22201363742351532] and bias 0.00034122366923838854
Batch 2300 loss is 0.019731326028704643
At batch 2300 layer 0 has weights [0.7909106016159058, 0.20934779942035675] and bias -0.00015062571037560701
Batch 2400 loss is 0.018757715821266174
At batch 2400 layer 0 has weights [0.8022904396057129, 0.19828154146671295] and bias -3.17542435368523e-05
Batch 2500 loss is 0.02328810840845108
At batch 2500 layer 0 has weights [0.8143905997276306, 0.18548676371574402] and bias 0.0001670441561145708
Batch 2600 loss is 0.011020500212907791
At batch 2600 layer 0 has weights [0.8263898491859436, 0.17322185635566711] and bias 3.1587371722707758e-06
Batch 2700 loss is 0.015119754709303379
At batch 2700 layer 0 has weights [0.8394449353218079, 0.16077207028865814] and bias 8.258635716629215e-06
Batch 2800 loss is 0.013951655477285385
At batch 2800 layer 0 has weights [0.8521954417228699, 0.14789293706417084] and bias -0.00013361399760469794
Batch 2900 loss is 0.012835938483476639
At batch 2900 layer 0 has weights [0.8639429211616516, 0.13584162294864655] and bias 7.064494275255129e-05
Batch 3000 loss is 0.0039777387864887714
At batch 3000 layer 0 has weights [0.8764307498931885, 0.12362632900476456] and bias 9.921917080646381e-05
Batch 3100 loss is 0.01764722540974617
At batch 3100 layer 0 has weights [0.8883024454116821, 0.111875519156456] and bias -0.0002834549522958696
Batch 3200 loss is 0.006298899184912443
At batch 3200 layer 0 has weights [0.9013157486915588, 0.09869007766246796] and bias -0.00013349956134334207
Batch 3300 loss is 0.005489571485668421
At batch 3300 layer 0 has weights [0.9127153754234314, 0.087285116314888] and bias 3.8936661439947784e-05
Batch 3400 loss is 0.009458016604185104
At batch 3400 layer 0 has weights [0.9248656034469604, 0.07500891387462616] and bias 1.4588769090551068e-06
Batch 3500 loss is 0.006184670142829418
At batch 3500 layer 0 has weights [0.9366987347602844, 0.06282132863998413] and bias 1.92165589396609e-05
Batch 3600 loss is 0.009554645046591759
At batch 3600 layer 0 has weights [0.9493486285209656, 0.050485942512750626] and bias 0.00016642650007270277
Batch 3700 loss is 0.006065245717763901
At batch 3700 layer 0 has weights [0.9614222645759583, 0.03859681636095047] and bias 4.670099951908924e-05
Batch 3800 loss is 0.005040871445089579
At batch 3800 layer 0 has weights [0.9733586311340332, 0.02660825289785862] and bias -3.628941340139136e-05
Batch 3900 loss is 0.0022769817151129246
At batch 3900 layer 0 has weights [0.9860929250717163, 0.014168164692819118] and bias -6.992403359618038e-05
Batch 4000 loss is 0.00037584188976325095
At batch 4000 layer 0 has weights [0.9988380074501038, 0.000632126466371119] and bias 0.0001628917525522411
Batch 4100 loss is 0.0003195672470610589
At batch 4100 layer 0 has weights [1.0000468492507935, 0.0001860251504695043] and bias -0.0003923241456504911
Batch 4200 loss is 0.0002756127214524895
At batch 4200 layer 0 has weights [1.0002630949020386, 0.00026757665909826756] and bias -0.0004050537245348096
Batch 4300 loss is 0.00040061684558168054
At batch 4300 layer 0 has weights [1.0003712177276611, 0.0004841129994019866] and bias -3.869841748382896e-05
Batch 4400 loss is 0.00043645166442729533
At batch 4400 layer 0 has weights [0.9997562766075134, -0.0002556735125835985] and bias -0.00021908360940869898
Batch 4500 loss is 0.0001798612647689879
At batch 4500 layer 0 has weights [1.0001453161239624, -4.734862523037009e-05] and bias -0.00023931375471875072
Batch 4600 loss is 8.667078509461135e-05
At batch 4600 layer 0 has weights [1.0000523328781128, 0.0001476805191487074] and bias -1.811593392631039e-05
Batch 4700 loss is 9.331201727036387e-05
At batch 4700 layer 0 has weights [0.9998999834060669, -0.00011545522283995524] and bias 7.398614980047569e-05
Batch 4800 loss is 0.0003584159421734512
At batch 4800 layer 0 has weights [0.9995422959327698, -0.0001798082230379805] and bias -0.00013057320029474795
Batch 4900 loss is 1.967684511328116e-05
At batch 4900 layer 0 has weights [0.9999895095825195, -2.7812146072392352e-05] and bias 1.0221824595646467e-05
Batch 5000 loss is 0.00026981462724506855
At batch 5000 layer 0 has weights [0.9999750852584839, -0.00022018063464201987] and bias -0.0002879830717574805
