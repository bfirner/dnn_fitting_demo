Batch 0 loss is 1.1836212873458862
At batch 0 layer 0 has weights [0.14392328262329102, 0.03108280897140503] and bias -0.6683591604232788
Batch 100 loss is 0.9334238767623901
At batch 100 layer 0 has weights [0.24156218767166138, 0.1299184113740921] and bias -0.568360447883606
Batch 200 loss is 0.712084949016571
At batch 200 layer 0 has weights [0.34044167399406433, 0.22929854691028595] and bias -0.4683617353439331
Batch 300 loss is 0.44116902351379395
At batch 300 layer 0 has weights [0.4391022026538849, 0.32752135396003723] and bias -0.36836302280426025
Batch 400 loss is 0.2891412377357483
At batch 400 layer 0 has weights [0.5402891039848328, 0.4070589542388916] and bias -0.28054672479629517
Batch 500 loss is 0.1572861671447754
At batch 500 layer 0 has weights [0.6399879455566406, 0.48226869106292725] and bias -0.19325868785381317
Batch 600 loss is 0.10880512744188309
At batch 600 layer 0 has weights [0.661546528339386, 0.48531320691108704] and bias -0.14737839996814728
Batch 700 loss is 0.12143971025943756
At batch 700 layer 0 has weights [0.6539276838302612, 0.46129557490348816] and bias -0.11539958417415619
Batch 800 loss is 0.07060351967811584
At batch 800 layer 0 has weights [0.645013689994812, 0.4361063838005066] and bias -0.08229365199804306
Batch 900 loss is 0.01678183116018772
At batch 900 layer 0 has weights [0.6359814405441284, 0.41053783893585205] and bias -0.04731009900569916
Batch 1000 loss is 0.04939693212509155
At batch 1000 layer 0 has weights [0.6269587278366089, 0.38447076082229614] and bias -0.011784976348280907
Batch 1100 loss is 0.03493525832891464
At batch 1100 layer 0 has weights [0.6303737759590149, 0.3701186776161194] and bias -9.986289660446346e-05
Batch 1200 loss is 0.04529903456568718
At batch 1200 layer 0 has weights [0.6395514607429504, 0.3602868914604187] and bias -0.00027007603785023093
Batch 1300 loss is 0.03349766135215759
At batch 1300 layer 0 has weights [0.6484066843986511, 0.35106077790260315] and bias -0.0003112618578597903
Batch 1400 loss is 0.010983584448695183
At batch 1400 layer 0 has weights [0.6578310132026672, 0.3426096737384796] and bias -0.0005305621307343245
Batch 1500 loss is 0.041790805757045746
At batch 1500 layer 0 has weights [0.6667353510856628, 0.33285820484161377] and bias 0.00012446312757674605
Batch 1600 loss is 0.06069943681359291
At batch 1600 layer 0 has weights [0.6771488785743713, 0.3230381906032562] and bias 3.0414910725085065e-05
Batch 1700 loss is 0.010207329876720905
At batch 1700 layer 0 has weights [0.6862916350364685, 0.31316521763801575] and bias -8.689182868693024e-05
Batch 1800 loss is 0.03820362314581871
At batch 1800 layer 0 has weights [0.696175217628479, 0.30360710620880127] and bias 0.0003479626902844757
Batch 1900 loss is 0.036991264671087265
At batch 1900 layer 0 has weights [0.7053192853927612, 0.2941894829273224] and bias 0.0001558669173391536
Batch 2000 loss is 0.009000233374536037
At batch 2000 layer 0 has weights [0.7158685922622681, 0.2843235433101654] and bias 3.704267510329373e-05
Batch 2100 loss is 0.04319595918059349
At batch 2100 layer 0 has weights [0.7251124978065491, 0.2748546898365021] and bias 0.0002611797535791993
Batch 2200 loss is 0.025068560615181923
At batch 2200 layer 0 has weights [0.734962522983551, 0.26544445753097534] and bias -6.103888154029846e-05
Batch 2300 loss is 0.03205469995737076
At batch 2300 layer 0 has weights [0.7445054650306702, 0.2557030916213989] and bias -0.00027019521803595126
Batch 2400 loss is 0.038729287683963776
At batch 2400 layer 0 has weights [0.7547800540924072, 0.24582785367965698] and bias -1.1428246580180712e-05
Batch 2500 loss is 0.02960936166346073
At batch 2500 layer 0 has weights [0.7641866207122803, 0.23605133593082428] and bias -9.323234735347796e-06
Batch 2600 loss is 0.02143109031021595
At batch 2600 layer 0 has weights [0.7741785049438477, 0.22570295631885529] and bias -0.00027935177786275744
Batch 2700 loss is 0.038064438849687576
At batch 2700 layer 0 has weights [0.783509373664856, 0.21656721830368042] and bias -0.00010530390136409551
Batch 2800 loss is 0.013156207278370857
At batch 2800 layer 0 has weights [0.7921951413154602, 0.20803995430469513] and bias -0.0002932209463324398
Batch 2900 loss is 0.019293993711471558
At batch 2900 layer 0 has weights [0.8012903332710266, 0.19899442791938782] and bias 0.00048696965677663684
Batch 3000 loss is 0.02392367273569107
At batch 3000 layer 0 has weights [0.8115060329437256, 0.18815118074417114] and bias -0.0002969074121210724
Batch 3100 loss is 0.017075449228286743
At batch 3100 layer 0 has weights [0.8207385540008545, 0.17880086600780487] and bias -6.620102794840932e-05
Batch 3200 loss is 0.026875745505094528
At batch 3200 layer 0 has weights [0.8301781415939331, 0.16952140629291534] and bias -0.0003458616556599736
Batch 3300 loss is 0.020350048318505287
At batch 3300 layer 0 has weights [0.8390491604804993, 0.16036492586135864] and bias 0.00020102126291021705
Batch 3400 loss is 0.00021057119010947645
At batch 3400 layer 0 has weights [0.8481890559196472, 0.15174299478530884] and bias -0.00017021960229612887
Batch 3500 loss is 0.01402374915778637
At batch 3500 layer 0 has weights [0.8572238087654114, 0.14207477867603302] and bias -0.0004350984236225486
Batch 3600 loss is 0.01673494279384613
At batch 3600 layer 0 has weights [0.8672201633453369, 0.13259804248809814] and bias -9.238209167961031e-05
Batch 3700 loss is 0.004188953898847103
At batch 3700 layer 0 has weights [0.8766215443611145, 0.12376252561807632] and bias 0.0001533592730993405
Batch 3800 loss is 0.007465031463652849
At batch 3800 layer 0 has weights [0.8862905502319336, 0.11386048793792725] and bias 0.00027323781978338957
Batch 3900 loss is 0.01342905592173338
At batch 3900 layer 0 has weights [0.8947156071662903, 0.10456837713718414] and bias 1.7630940419621766e-05
Batch 4000 loss is 0.03446711599826813
At batch 4000 layer 0 has weights [0.9052516222000122, 0.09512171149253845] and bias 6.173019210109487e-05
Batch 4100 loss is 0.008364775218069553
At batch 4100 layer 0 has weights [0.9133976697921753, 0.08601732552051544] and bias 0.00016244800644926727
Batch 4200 loss is 0.00972381979227066
At batch 4200 layer 0 has weights [0.9228423237800598, 0.07690220326185226] and bias 0.0002167135098716244
Batch 4300 loss is 0.0006242045201361179
At batch 4300 layer 0 has weights [0.9330703616142273, 0.06770025193691254] and bias 0.0002389447035966441
Batch 4400 loss is 0.0073669529519975185
At batch 4400 layer 0 has weights [0.9416394829750061, 0.05803609639406204] and bias 2.9494196496671066e-05
Batch 4500 loss is 0.007719231769442558
At batch 4500 layer 0 has weights [0.9516861438751221, 0.04862626641988754] and bias -0.00010042360372608528
Batch 4600 loss is 0.03015453740954399
At batch 4600 layer 0 has weights [0.9611753225326538, 0.03872361406683922] and bias 0.00016273958317469805
Batch 4700 loss is 0.004112548660486937
At batch 4700 layer 0 has weights [0.9709256887435913, 0.02977338805794716] and bias 4.134056143811904e-05
Batch 4800 loss is 0.0021159786265343428
At batch 4800 layer 0 has weights [0.979009211063385, 0.020672006532549858] and bias -5.966949538560584e-05
Batch 4900 loss is 0.0008426546701230109
At batch 4900 layer 0 has weights [0.9877827167510986, 0.012049330398440361] and bias 0.0001287802297156304
Batch 5000 loss is 0.031576063483953476
At batch 5000 layer 0 has weights [0.9971510171890259, 0.0030681942589581013] and bias 6.724411377945216e-06
