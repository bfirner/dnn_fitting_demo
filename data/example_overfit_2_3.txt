Batch 0 loss is 1.2004780769348145
At batch 0 layer 0 has weights [-0.02082914113998413, 0.06565392017364502] and bias -0.697144627571106
Batch 100 loss is 0.9884694814682007
At batch 100 layer 0 has weights [0.07868506759405136, 0.16526319086551666] and bias -0.5971459150314331
Batch 200 loss is 0.7167335152626038
At batch 200 layer 0 has weights [0.17841361463069916, 0.2648516595363617] and bias -0.49714720249176025
Batch 300 loss is 0.5779823064804077
At batch 300 layer 0 has weights [0.2762564718723297, 0.3620351552963257] and bias -0.3971484899520874
Batch 400 loss is 0.42914775013923645
At batch 400 layer 0 has weights [0.37406349182128906, 0.43756943941116333] and bias -0.3111969232559204
Batch 500 loss is 0.2472262978553772
At batch 500 layer 0 has weights [0.46963798999786377, 0.5058215856552124] and bias -0.2279103398323059
Batch 600 loss is 0.11052854359149933
At batch 600 layer 0 has weights [0.5661891102790833, 0.5817315578460693] and bias -0.1395847499370575
Batch 700 loss is 0.08283138275146484
At batch 700 layer 0 has weights [0.5565085411071777, 0.5534839630126953] and bias -0.10996512323617935
Batch 800 loss is 0.07630512863397598
At batch 800 layer 0 has weights [0.5494514107704163, 0.5281739830970764] and bias -0.07678474485874176
Batch 900 loss is 0.07632555812597275
At batch 900 layer 0 has weights [0.5414734482765198, 0.5019403696060181] and bias -0.04328456148505211
Batch 1000 loss is 0.09134724736213684
At batch 1000 layer 0 has weights [0.5337905287742615, 0.4756012558937073] and bias -0.008764705620706081
Batch 1100 loss is 0.029550708830356598
At batch 1100 layer 0 has weights [0.5379540920257568, 0.46144866943359375] and bias -0.00047033748705871403
Batch 1200 loss is 0.05665842816233635
At batch 1200 layer 0 has weights [0.5479245781898499, 0.4523366391658783] and bias -5.6989661970874295e-05
Batch 1300 loss is 0.0003433258389122784
At batch 1300 layer 0 has weights [0.5577452182769775, 0.44225263595581055] and bias -0.0003421808942221105
Batch 1400 loss is 0.027116689831018448
At batch 1400 layer 0 has weights [0.5682898759841919, 0.43194377422332764] and bias -2.08308883884456e-05
Batch 1500 loss is 0.052823517471551895
At batch 1500 layer 0 has weights [0.577614426612854, 0.42243775725364685] and bias -3.02830903819995e-05
Batch 1600 loss is 0.025791022926568985
At batch 1600 layer 0 has weights [0.5877785086631775, 0.4122283160686493] and bias -3.421696965233423e-05
Batch 1700 loss is 0.03771832585334778
At batch 1700 layer 0 has weights [0.5978147387504578, 0.4022054076194763] and bias -2.894029785238672e-05
Batch 1800 loss is 0.037037648260593414
At batch 1800 layer 0 has weights [0.6072741150856018, 0.3926350772380829] and bias -0.00021788447338622063
Batch 1900 loss is 0.024308614432811737
At batch 1900 layer 0 has weights [0.6171899437904358, 0.38299858570098877] and bias 0.00027688880800269544
Batch 2000 loss is 0.02354395017027855
At batch 2000 layer 0 has weights [0.6261705160140991, 0.3741886615753174] and bias -1.7102228184739943e-06
Batch 2100 loss is 0.06869027018547058
At batch 2100 layer 0 has weights [0.635299026966095, 0.36414042115211487] and bias -0.0002979897544719279
Batch 2200 loss is 0.02216731756925583
At batch 2200 layer 0 has weights [0.6461137533187866, 0.35372963547706604] and bias 0.0001165172434411943
Batch 2300 loss is 0.0323982909321785
At batch 2300 layer 0 has weights [0.6560176610946655, 0.34429481625556946] and bias -0.00018981257744599134
Batch 2400 loss is 0.0212404802441597
At batch 2400 layer 0 has weights [0.6655436754226685, 0.33511731028556824] and bias -0.00022706940944772214
Batch 2500 loss is 0.06096373870968819
At batch 2500 layer 0 has weights [0.67542964220047, 0.32473739981651306] and bias -3.0237011742428876e-05
Batch 2600 loss is 0.029921017587184906
At batch 2600 layer 0 has weights [0.6862117052078247, 0.3142895996570587] and bias 0.00017439034127164632
Batch 2700 loss is 0.028675127774477005
At batch 2700 layer 0 has weights [0.6958521604537964, 0.3042493164539337] and bias 0.00010420438047731295
Batch 2800 loss is 0.03673502057790756
At batch 2800 layer 0 has weights [0.7061811089515686, 0.2938326299190521] and bias -1.761940075084567e-05
Batch 2900 loss is 0.01779644563794136
At batch 2900 layer 0 has weights [0.715786874294281, 0.28414154052734375] and bias 5.024670099373907e-05
Batch 3000 loss is 0.03452393412590027
At batch 3000 layer 0 has weights [0.7253643274307251, 0.2741446793079376] and bias 0.00019046044326387346
Batch 3100 loss is 0.041913218796253204
At batch 3100 layer 0 has weights [0.7348765730857849, 0.2642410695552826] and bias -0.00022806036577094346
Batch 3200 loss is 0.016175828874111176
At batch 3200 layer 0 has weights [0.7447604537010193, 0.25564631819725037] and bias 7.276008091139374e-06
Batch 3300 loss is 0.00010321316949557513
At batch 3300 layer 0 has weights [0.7537370920181274, 0.24627640843391418] and bias 9.688261343399063e-05
Batch 3400 loss is 0.029643699526786804
At batch 3400 layer 0 has weights [0.7642815709114075, 0.2360503375530243] and bias -0.00013131240848451853
Batch 3500 loss is 0.00014380233187694103
At batch 3500 layer 0 has weights [0.7748474478721619, 0.2254248559474945] and bias 2.4687200493644923e-05
Batch 3600 loss is 0.020330369472503662
At batch 3600 layer 0 has weights [0.7842413783073425, 0.21548794209957123] and bias 7.968665158841759e-05
Batch 3700 loss is 0.012871019542217255
At batch 3700 layer 0 has weights [0.7944513559341431, 0.20550529658794403] and bias -6.031623797753127e-06
Batch 3800 loss is 0.018444638699293137
At batch 3800 layer 0 has weights [0.8039265871047974, 0.1962268352508545] and bias -0.00010858383757295087
Batch 3900 loss is 0.006223524454981089
At batch 3900 layer 0 has weights [0.8130307793617249, 0.18767794966697693] and bias -0.00014345448289532214
Batch 4000 loss is 0.03905196487903595
At batch 4000 layer 0 has weights [0.8220593333244324, 0.17779730260372162] and bias -0.00017083872808143497
Batch 4100 loss is 0.016321901232004166
At batch 4100 layer 0 has weights [0.8318032026290894, 0.1677294224500656] and bias -0.000393561931559816
Batch 4200 loss is 0.010139426216483116
At batch 4200 layer 0 has weights [0.8414788842201233, 0.15875202417373657] and bias 0.00010917945473920554
Batch 4300 loss is 0.009337957948446274
At batch 4300 layer 0 has weights [0.8515256643295288, 0.1483471691608429] and bias 2.0775933080585673e-05
Batch 4400 loss is 0.012966266833245754
At batch 4400 layer 0 has weights [0.8620849847793579, 0.13798850774765015] and bias -1.2905016774311662e-05
Batch 4500 loss is 0.02394573949277401
At batch 4500 layer 0 has weights [0.8724142909049988, 0.12763193249702454] and bias -1.8358618035563268e-05
Batch 4600 loss is 0.007813128642737865
At batch 4600 layer 0 has weights [0.8825729489326477, 0.11825951188802719] and bias 5.7732610002858564e-05
Batch 4700 loss is 0.020540941506624222
At batch 4700 layer 0 has weights [0.8912283778190613, 0.10830360651016235] and bias 0.000262749643297866
Batch 4800 loss is 0.015594156458973885
At batch 4800 layer 0 has weights [0.9009259343147278, 0.09884601086378098] and bias -4.119276491110213e-05
Batch 4900 loss is 0.005729068070650101
At batch 4900 layer 0 has weights [0.9118279218673706, 0.08827709406614304] and bias 0.00017235871928278357
Batch 5000 loss is 0.012530576437711716
At batch 5000 layer 0 has weights [0.922095000743866, 0.07858394086360931] and bias -0.00021069827198516577
