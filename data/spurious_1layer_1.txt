Batch 0 loss is 1.2523996829986572
At batch 0 layer 0 has weights [-0.5490058660507202, -0.3096061050891876] and bias -0.4198761582374573
Batch 100 loss is 1.273305892944336
At batch 100 layer 0 has weights [-0.4499914050102234, -0.21141989529132843] and bias -0.3198774456977844
Batch 200 loss is 0.9003022313117981
At batch 200 layer 0 has weights [-0.35106539726257324, -0.1159682348370552] and bias -0.21987827122211456
Batch 300 loss is 0.7520302534103394
At batch 300 layer 0 has weights [-0.25032344460487366, -0.015978125855326653] and bias -0.11987806856632233
Batch 400 loss is 0.6257905960083008
At batch 400 layer 0 has weights [-0.15006175637245178, 0.08199433237314224] and bias -0.019877992570400238
Batch 500 loss is 0.5609970688819885
At batch 500 layer 0 has weights [-0.0522286556661129, 0.1779613047838211] and bias 0.008415601216256618
Batch 600 loss is 0.32814961671829224
At batch 600 layer 0 has weights [0.04475383087992668, 0.27441588044166565] and bias 0.006958606652915478
Batch 700 loss is 0.2732263207435608
At batch 700 layer 0 has weights [0.14633215963840485, 0.37811100482940674] and bias 0.011808283627033234
Batch 800 loss is 0.20850205421447754
At batch 800 layer 0 has weights [0.24343650043010712, 0.47446882724761963] and bias 0.010709344409406185
Batch 900 loss is 0.12289629131555557
At batch 900 layer 0 has weights [0.3415127992630005, 0.5718084573745728] and bias 0.01101588923484087
Batch 1000 loss is 0.07647575438022614
At batch 1000 layer 0 has weights [0.3912462294101715, 0.6086651682853699] and bias 0.0005529723130166531
Batch 1100 loss is 0.037640176713466644
At batch 1100 layer 0 has weights [0.4016997516155243, 0.5982760190963745] and bias 0.00029113213531672955
Batch 1200 loss is 0.055456582456827164
At batch 1200 layer 0 has weights [0.4133816659450531, 0.5864046216011047] and bias -0.00035425886744633317
Batch 1300 loss is 0.01804911345243454
At batch 1300 layer 0 has weights [0.42406386137008667, 0.5758806467056274] and bias 8.41243818285875e-05
Batch 1400 loss is 0.03537642955780029
At batch 1400 layer 0 has weights [0.4345812201499939, 0.565335750579834] and bias 6.428354390664026e-05
Batch 1500 loss is 0.06948288530111313
At batch 1500 layer 0 has weights [0.44592684507369995, 0.554483950138092] and bias -0.00030317442724481225
Batch 1600 loss is 0.05094458907842636
At batch 1600 layer 0 has weights [0.45841366052627563, 0.5415874123573303] and bias 0.00020980801491532475
Batch 1700 loss is 0.0828847736120224
At batch 1700 layer 0 has weights [0.4703308939933777, 0.5293070077896118] and bias 0.00017312131240032613
Batch 1800 loss is 0.016608787700533867
At batch 1800 layer 0 has weights [0.48244956135749817, 0.5178260803222656] and bias -0.0004956278135068715
Batch 1900 loss is 0.031733814626932144
At batch 1900 layer 0 has weights [0.4945629835128784, 0.5051517486572266] and bias 0.00018651221762411296
Batch 2000 loss is 0.015558446757495403
At batch 2000 layer 0 has weights [0.5082013607025146, 0.49194276332855225] and bias 0.00013992043386679143
Batch 2100 loss is 0.03016999363899231
At batch 2100 layer 0 has weights [0.5192137360572815, 0.4805087149143219] and bias 0.00013432500418275595
Batch 2200 loss is 0.0294034406542778
At batch 2200 layer 0 has weights [0.5315538644790649, 0.46835488080978394] and bias 0.0001988975127460435
Batch 2300 loss is 0.028816794976592064
At batch 2300 layer 0 has weights [0.5428124070167542, 0.4572119116783142] and bias -0.000253961916314438
Batch 2400 loss is 0.06985671818256378
At batch 2400 layer 0 has weights [0.5538230538368225, 0.4458918273448944] and bias -1.683112895989325e-05
Batch 2500 loss is 0.04084853082895279
At batch 2500 layer 0 has weights [0.5662106871604919, 0.4339613914489746] and bias -0.00024533591931685805
Batch 2600 loss is 0.0399165116250515
At batch 2600 layer 0 has weights [0.5781157612800598, 0.4228903353214264] and bias 6.208778358995914e-05
Batch 2700 loss is 0.11560817062854767
At batch 2700 layer 0 has weights [0.5894874930381775, 0.4101264774799347] and bias 5.885704740649089e-05
Batch 2800 loss is 0.0875115618109703
At batch 2800 layer 0 has weights [0.6022993326187134, 0.39827486872673035] and bias 0.00046811209176667035
Batch 2900 loss is 0.024191394448280334
At batch 2900 layer 0 has weights [0.6152312159538269, 0.3850392699241638] and bias -5.605578735412564e-06
Batch 3000 loss is 0.07002503424882889
At batch 3000 layer 0 has weights [0.6266896724700928, 0.37320825457572937] and bias 1.0149040463147685e-05
Batch 3100 loss is 0.045386746525764465
At batch 3100 layer 0 has weights [0.6385299563407898, 0.36155134439468384] and bias 0.0002198752627009526
Batch 3200 loss is 0.032782379537820816
At batch 3200 layer 0 has weights [0.6510618925094604, 0.34875762462615967] and bias 2.785665128612891e-05
Batch 3300 loss is 0.021297238767147064
At batch 3300 layer 0 has weights [0.6621288657188416, 0.337456613779068] and bias 0.00019879976753145456
Batch 3400 loss is 0.020626818761229515
At batch 3400 layer 0 has weights [0.6741244196891785, 0.3257160484790802] and bias -0.000199771806364879
Batch 3500 loss is 0.039619214832782745
At batch 3500 layer 0 has weights [0.6862453818321228, 0.3139584958553314] and bias 0.0004397502343636006
Batch 3600 loss is 0.01925589144229889
At batch 3600 layer 0 has weights [0.6978200674057007, 0.30180665850639343] and bias -0.00017135431698989123
Batch 3700 loss is 0.027440013363957405
At batch 3700 layer 0 has weights [0.7103040218353271, 0.28925949335098267] and bias -7.641615957254544e-05
Batch 3800 loss is 0.008720919489860535
At batch 3800 layer 0 has weights [0.7227676510810852, 0.27734601497650146] and bias -4.8432775656692684e-05
Batch 3900 loss is 0.025101397186517715
At batch 3900 layer 0 has weights [0.7346283197402954, 0.2654842436313629] and bias -0.00027555052656680346
Batch 4000 loss is 0.008207905106246471
At batch 4000 layer 0 has weights [0.7465603351593018, 0.252890944480896] and bias 0.0003726316790562123
Batch 4100 loss is 0.03816671669483185
At batch 4100 layer 0 has weights [0.7588547468185425, 0.24163058400154114] and bias 0.0004889180418103933
Batch 4200 loss is 0.02172147110104561
At batch 4200 layer 0 has weights [0.7719840407371521, 0.2286422848701477] and bias 0.00013548471906688064
Batch 4300 loss is 0.02704624831676483
At batch 4300 layer 0 has weights [0.7841958999633789, 0.215755894780159] and bias -4.5112508814781904e-05
Batch 4400 loss is 0.019297167658805847
At batch 4400 layer 0 has weights [0.7959457635879517, 0.204288512468338] and bias 9.753182530403137e-05
Batch 4500 loss is 0.012227400206029415
At batch 4500 layer 0 has weights [0.8073305487632751, 0.19245824217796326] and bias -0.00010635390208335593
Batch 4600 loss is 0.022831305861473083
At batch 4600 layer 0 has weights [0.8188285827636719, 0.18089883029460907] and bias 0.00040551164420321584
Batch 4700 loss is 0.015998655930161476
At batch 4700 layer 0 has weights [0.831230878829956, 0.16915975511074066] and bias -4.526375778368674e-05
Batch 4800 loss is 0.024780793115496635
At batch 4800 layer 0 has weights [0.8424199819564819, 0.15755623579025269] and bias -0.00015074103430379182
Batch 4900 loss is 0.00459325173869729
At batch 4900 layer 0 has weights [0.8539553284645081, 0.1460488736629486] and bias -3.104840652667917e-05
Batch 5000 loss is 0.016953831538558006
At batch 5000 layer 0 has weights [0.8654259443283081, 0.13471417129039764] and bias 0.00014818261843174696
