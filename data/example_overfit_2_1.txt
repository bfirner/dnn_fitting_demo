Batch 0 loss is 0.6582998037338257
At batch 0 layer 0 has weights [-0.015556752681732178, 0.32220762968063354] and bias -0.4471096992492676
Batch 100 loss is 0.5830450057983398
At batch 100 layer 0 has weights [0.08515431731939316, 0.4152188301086426] and bias -0.3511775732040405
Batch 200 loss is 0.4119190573692322
At batch 200 layer 0 has weights [0.18147456645965576, 0.49158936738967896] and bias -0.262958824634552
Batch 300 loss is 0.2754670977592468
At batch 300 layer 0 has weights [0.2791300415992737, 0.5766183137893677] and bias -0.16900812089443207
Batch 400 loss is 0.12095072120428085
At batch 400 layer 0 has weights [0.37842416763305664, 0.6656240224838257] and bias -0.07418973743915558
Batch 500 loss is 0.036619383841753006
At batch 500 layer 0 has weights [0.38506510853767395, 0.6497401595115662] and bias -0.03451172634959221
Batch 600 loss is 0.039124779403209686
At batch 600 layer 0 has weights [0.3775199055671692, 0.6219046115875244] and bias 0.0005436951178126037
Batch 700 loss is 0.038560137152671814
At batch 700 layer 0 has weights [0.38772064447402954, 0.6123936772346497] and bias 0.00022477819584310055
Batch 800 loss is 0.056906118988990784
At batch 800 layer 0 has weights [0.3980419635772705, 0.6026029586791992] and bias 0.00010977903002640232
Batch 900 loss is 0.03729724511504173
At batch 900 layer 0 has weights [0.4073128402233124, 0.5925320982933044] and bias -0.0002019963867496699
Batch 1000 loss is 0.09129635989665985
At batch 1000 layer 0 has weights [0.4170377850532532, 0.5831669569015503] and bias -0.00036847961018793285
Batch 1100 loss is 0.03597164899110794
At batch 1100 layer 0 has weights [0.42681145668029785, 0.5734962224960327] and bias -0.00017447455320507288
Batch 1200 loss is 0.03568459302186966
At batch 1200 layer 0 has weights [0.43594032526016235, 0.5643060207366943] and bias 0.00027694733580574393
Batch 1300 loss is 0.06953606754541397
At batch 1300 layer 0 has weights [0.4464587867259979, 0.5537207126617432] and bias 0.00026485483977012336
Batch 1400 loss is 0.0681397020816803
At batch 1400 layer 0 has weights [0.4559779167175293, 0.5436654090881348] and bias 0.0001274606038350612
Batch 1500 loss is 0.03375723212957382
At batch 1500 layer 0 has weights [0.4659608006477356, 0.5337448120117188] and bias -0.0002763417724054307
Batch 1600 loss is 0.03302277624607086
At batch 1600 layer 0 has weights [0.47620347142219543, 0.5238960981369019] and bias -0.00035471664159558713
Batch 1700 loss is 0.03230919688940048
At batch 1700 layer 0 has weights [0.48627474904060364, 0.5141103267669678] and bias -5.0882164941867813e-05
Batch 1800 loss is 0.047372862696647644
At batch 1800 layer 0 has weights [0.49568045139312744, 0.5044631361961365] and bias 3.1684248824603856e-06
Batch 1900 loss is 0.015749666839838028
At batch 1900 layer 0 has weights [0.50581294298172, 0.4945603311061859] and bias 0.0001313564571319148
Batch 2000 loss is 0.03103490360081196
At batch 2000 layer 0 has weights [0.5151311159133911, 0.4852868318557739] and bias 0.0004562991380225867
Batch 2100 loss is 0.044838063418865204
At batch 2100 layer 0 has weights [0.5247099995613098, 0.4757712483406067] and bias 2.395808041910641e-05
Batch 2200 loss is 0.029344875365495682
At batch 2200 layer 0 has weights [0.5332751870155334, 0.4665760397911072] and bias -0.00011982266005361453
Batch 2300 loss is 0.08573257923126221
At batch 2300 layer 0 has weights [0.5434653759002686, 0.45623844861984253] and bias -9.327286534244195e-05
Batch 2400 loss is 0.028075575828552246
At batch 2400 layer 0 has weights [0.5535731315612793, 0.44674578309059143] and bias -4.387697845231742e-05
Batch 2500 loss is 0.02744322642683983
At batch 2500 layer 0 has weights [0.563126266002655, 0.43691369891166687] and bias -0.00017837146879173815
Batch 2600 loss is 0.013460962101817131
At batch 2600 layer 0 has weights [0.5727806687355042, 0.42700979113578796] and bias 7.731966434221249e-06
Batch 2700 loss is 0.026282096281647682
At batch 2700 layer 0 has weights [0.5818317532539368, 0.4184613525867462] and bias -4.747721322928555e-05
Batch 2800 loss is 0.03866441175341606
At batch 2800 layer 0 has weights [0.590668797492981, 0.4088670313358307] and bias -0.00012435740791261196
Batch 2900 loss is 0.08710990846157074
At batch 2900 layer 0 has weights [0.6018640398979187, 0.3980639576911926] and bias -7.414239462377736e-06
Batch 3000 loss is 0.04856053739786148
At batch 3000 layer 0 has weights [0.612303614616394, 0.387433797121048] and bias 4.984077895642258e-05
Batch 3100 loss is 0.023774193599820137
At batch 3100 layer 0 has weights [0.6209717392921448, 0.37885016202926636] and bias 3.295506030553952e-05
Batch 3200 loss is 0.06953273713588715
At batch 3200 layer 0 has weights [0.6304865479469299, 0.3693084418773651] and bias -0.00028559149359352887
Batch 3300 loss is 0.07920635491609573
At batch 3300 layer 0 has weights [0.640174925327301, 0.3600146770477295] and bias 0.00034652321483008564
Batch 3400 loss is 0.044099994003772736
At batch 3400 layer 0 has weights [0.6499096155166626, 0.34992244839668274] and bias -0.00039560534060001373
Batch 3500 loss is 0.04294045269489288
At batch 3500 layer 0 has weights [0.6597773432731628, 0.34044232964515686] and bias 0.0003165148082189262
Batch 3600 loss is 0.03165147081017494
At batch 3600 layer 0 has weights [0.6677959561347961, 0.33170291781425476] and bias -0.0004316775011830032
Batch 3700 loss is 0.04039321094751358
At batch 3700 layer 0 has weights [0.6787692308425903, 0.3213427662849426] and bias -0.0003658515925053507
Batch 3800 loss is 0.058582160621881485
At batch 3800 layer 0 has weights [0.6883286833763123, 0.31131798028945923] and bias -0.0001063943564076908
Batch 3900 loss is 0.04735079035162926
At batch 3900 layer 0 has weights [0.69757080078125, 0.30231183767318726] and bias 0.00017322233179584146
Batch 4000 loss is 0.027674110606312752
At batch 4000 layer 0 has weights [0.707107663154602, 0.29316335916519165] and bias 4.605233698384836e-05
Batch 4100 loss is 0.026683596894145012
At batch 4100 layer 0 has weights [0.7177159190177917, 0.28269845247268677] and bias 1.2279822840355337e-05
Batch 4200 loss is 0.025737697258591652
At batch 4200 layer 0 has weights [0.7285143136978149, 0.2718872129917145] and bias -6.046587259334046e-06
Batch 4300 loss is 0.0165601447224617
At batch 4300 layer 0 has weights [0.7377899885177612, 0.2618035674095154] and bias 0.00022645153512712568
Batch 4400 loss is 0.02396738901734352
At batch 4400 layer 0 has weights [0.7468929886817932, 0.2533770501613617] and bias 9.518623846815899e-05
Batch 4500 loss is 0.03084735944867134
At batch 4500 layer 0 has weights [0.7559024691581726, 0.24431656301021576] and bias 0.0002120061544701457
Batch 4600 loss is 0.022395316511392593
At batch 4600 layer 0 has weights [0.7656667828559875, 0.23378531634807587] and bias -0.00022998699569143355
Batch 4700 loss is 0.02809295430779457
At batch 4700 layer 0 has weights [0.7763171195983887, 0.22389493882656097] and bias 6.679035323031712e-06
Batch 4800 loss is 0.013681353069841862
At batch 4800 layer 0 has weights [0.7862339615821838, 0.21430334448814392] and bias -5.92081414652057e-05
Batch 4900 loss is 1.6442545529571362e-05
At batch 4900 layer 0 has weights [0.7955331802368164, 0.20448589324951172] and bias -2.83495792245958e-05
Batch 5000 loss is 0.018786581233143806
At batch 5000 layer 0 has weights [0.8058337569236755, 0.19494420289993286] and bias 0.0001266499748453498
