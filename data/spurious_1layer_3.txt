Batch 0 loss is 0.8265788555145264
At batch 0 layer 0 has weights [0.23890191316604614, 0.3616005778312683] and bias -0.5792614817619324
Batch 100 loss is 0.5877656936645508
At batch 100 layer 0 has weights [0.33903613686561584, 0.46016886830329895] and bias -0.4792627692222595
Batch 200 loss is 0.47037091851234436
At batch 200 layer 0 has weights [0.4363167881965637, 0.5555996298789978] and bias -0.37926405668258667
Batch 300 loss is 0.2515300512313843
At batch 300 layer 0 has weights [0.5323200225830078, 0.6499689817428589] and bias -0.2792653441429138
Batch 400 loss is 0.1693844199180603
At batch 400 layer 0 has weights [0.5567901730537415, 0.6574270129203796] and bias -0.21439750492572784
Batch 500 loss is 0.12991100549697876
At batch 500 layer 0 has weights [0.5450160503387451, 0.6191766858100891] and bias -0.16422158479690552
Batch 600 loss is 0.11764682829380035
At batch 600 layer 0 has weights [0.5303804874420166, 0.5805920958518982] and bias -0.11081551015377045
Batch 700 loss is 0.08008012920618057
At batch 700 layer 0 has weights [0.5165674686431885, 0.5391079187393188] and bias -0.05552021414041519
Batch 800 loss is 0.06267818063497543
At batch 800 layer 0 has weights [0.5008070468902588, 0.49844256043434143] and bias 0.00033761438680812716
Batch 900 loss is 0.06094701588153839
At batch 900 layer 0 has weights [0.513215184211731, 0.4864901304244995] and bias 0.00010100535291712731
Batch 1000 loss is 0.07431094348430634
At batch 1000 layer 0 has weights [0.524987518787384, 0.47504231333732605] and bias 0.00011230298696318641
Batch 1100 loss is 0.04364475607872009
At batch 1100 layer 0 has weights [0.5374519228935242, 0.462523877620697] and bias -0.00027180134202353656
Batch 1200 loss is 0.07051053643226624
At batch 1200 layer 0 has weights [0.54921954870224, 0.4508594274520874] and bias -0.00011060020187869668
Batch 1300 loss is 0.04129238799214363
At batch 1300 layer 0 has weights [0.5607292056083679, 0.4394872486591339] and bias -6.805934390285984e-05
Batch 1400 loss is 0.0537857785820961
At batch 1400 layer 0 has weights [0.5729643106460571, 0.4262382686138153] and bias -0.0001322224852629006
Batch 1500 loss is 0.038952965289354324
At batch 1500 layer 0 has weights [0.5848802924156189, 0.4150642156600952] and bias 6.077210127841681e-05
Batch 1600 loss is 0.03799664229154587
At batch 1600 layer 0 has weights [0.5960184931755066, 0.40391048789024353] and bias -9.895994298858568e-05
Batch 1700 loss is 0.024682076647877693
At batch 1700 layer 0 has weights [0.6079437136650085, 0.39164817333221436] and bias 0.00012870284263044596
Batch 1800 loss is 0.012204638682305813
At batch 1800 layer 0 has weights [0.6201279163360596, 0.3799882233142853] and bias -0.00038080086233094335
Batch 1900 loss is 0.057550620287656784
At batch 1900 layer 0 has weights [0.6318694353103638, 0.3681623637676239] and bias 2.512001447030343e-05
Batch 2000 loss is 0.044673189520835876
At batch 2000 layer 0 has weights [0.6441097259521484, 0.35565248131752014] and bias -0.00010516305337660015
Batch 2100 loss is 0.04293695092201233
At batch 2100 layer 0 has weights [0.6571409702301025, 0.34266313910484314] and bias 0.00013059152115602046
Batch 2200 loss is 0.031094800680875778
At batch 2200 layer 0 has weights [0.6690376400947571, 0.33096182346343994] and bias 8.282386261271313e-05
Batch 2300 loss is 0.010191550478339195
At batch 2300 layer 0 has weights [0.6811122298240662, 0.3185519576072693] and bias -5.838965080329217e-05
Batch 2400 loss is 0.009630803018808365
At batch 2400 layer 0 has weights [0.6941066980361938, 0.30574047565460205] and bias 5.6087661505443975e-05
Batch 2500 loss is 0.02781744860112667
At batch 2500 layer 0 has weights [0.7051180601119995, 0.29442933201789856] and bias 4.041775900986977e-05
Batch 2600 loss is 0.00014319780166260898
At batch 2600 layer 0 has weights [0.7181925773620605, 0.2821418046951294] and bias 3.8685404433635995e-05
Batch 2700 loss is 0.042167603969573975
At batch 2700 layer 0 has weights [0.7306433320045471, 0.26918140053749084] and bias -2.585870970506221e-05
Batch 2800 loss is 0.024413660168647766
At batch 2800 layer 0 has weights [0.7435557246208191, 0.2571104168891907] and bias 0.00012476282427087426
Batch 2900 loss is 0.030607150867581367
At batch 2900 layer 0 has weights [0.7559964656829834, 0.24433043599128723] and bias -1.2166795386292506e-05
Batch 3000 loss is 0.03657400235533714
At batch 3000 layer 0 has weights [0.7667577862739563, 0.23279140889644623] and bias 0.00035899339127354324
Batch 3100 loss is 0.02106187865138054
At batch 3100 layer 0 has weights [0.7790347337722778, 0.22034317255020142] and bias 6.12604126217775e-05
Batch 3200 loss is 0.019651303067803383
At batch 3200 layer 0 has weights [0.7910552024841309, 0.2088739573955536] and bias -3.395009116502479e-05
Batch 3300 loss is 0.012363703921437263
At batch 3300 layer 0 has weights [0.8032939434051514, 0.1967363804578781] and bias 6.219753413461149e-05
Batch 3400 loss is 0.011717117391526699
At batch 3400 layer 0 has weights [0.8160130977630615, 0.18349424004554749] and bias 0.00026541773695498705
Batch 3500 loss is 0.010834787040948868
At batch 3500 layer 0 has weights [0.8285462856292725, 0.17167311906814575] and bias 2.626957939355634e-05
Batch 3600 loss is 0.010079680941998959
At batch 3600 layer 0 has weights [0.8401288986206055, 0.15965650975704193] and bias 9.057529496203642e-06
Batch 3700 loss is 0.01858670450747013
At batch 3700 layer 0 has weights [0.8524971604347229, 0.1471722424030304] and bias 6.575002043973655e-05
Batch 3800 loss is 0.008734001778066158
At batch 3800 layer 0 has weights [0.8641604781150818, 0.13577689230442047] and bias 0.0003169368428643793
Batch 3900 loss is 0.011726874858140945
At batch 3900 layer 0 has weights [0.8760711550712585, 0.12406081706285477] and bias -0.00015391639317385852
Batch 4000 loss is 0.007051748689264059
At batch 4000 layer 0 has weights [0.8886536955833435, 0.11113450676202774] and bias 0.0001054090607794933
Batch 4100 loss is 0.0062828087247908115
At batch 4100 layer 0 has weights [0.9014301300048828, 0.09831932187080383] and bias -1.2551568033813965e-05
Batch 4200 loss is 0.011312050744891167
At batch 4200 layer 0 has weights [0.9131957292556763, 0.08780387789011002] and bias -0.00029557241941802204
Batch 4300 loss is 0.007419664412736893
At batch 4300 layer 0 has weights [0.9242940545082092, 0.07528425008058548] and bias -0.000150911437231116
Batch 4400 loss is 0.008230149745941162
At batch 4400 layer 0 has weights [0.9375581741333008, 0.06307734549045563] and bias 0.000248818367253989
Batch 4500 loss is 0.0032410554122179747
At batch 4500 layer 0 has weights [0.9493628740310669, 0.05056513473391533] and bias 0.0001231143978657201
Batch 4600 loss is 0.0048369355499744415
At batch 4600 layer 0 has weights [0.9624437689781189, 0.03715604916214943] and bias 7.753910904284567e-05
Batch 4700 loss is 0.0044588353484869
At batch 4700 layer 0 has weights [0.9733635783195496, 0.026218149811029434] and bias -0.00015312116011045873
Batch 4800 loss is 0.0034603297244757414
At batch 4800 layer 0 has weights [0.9846538305282593, 0.015368427149951458] and bias 0.0001725680340314284
Batch 4900 loss is 0.0005126373143866658
At batch 4900 layer 0 has weights [0.9975160956382751, 0.002848599571734667] and bias 0.0002461537951603532
Batch 5000 loss is 0.0006332800257951021
At batch 5000 layer 0 has weights [1.0002094507217407, 0.0002712690329644829] and bias 0.0002026755246333778
