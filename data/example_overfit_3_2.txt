Batch 0 loss is 0.4779069423675537
At batch 0 layer 0 has weights [0.2551429271697998, -0.2307751476764679] and bias 0.5600447058677673
Batch 100 loss is 0.3411979675292969
At batch 100 layer 0 has weights [0.35375937819480896, -0.13548704981803894] and bias 0.5556555390357971
Batch 200 loss is 0.2469826489686966
At batch 200 layer 0 has weights [0.45083504915237427, -0.042152512818574905] and bias 0.5341882705688477
Batch 300 loss is 0.19198769330978394
At batch 300 layer 0 has weights [0.5184813737869263, 0.014552770182490349] and bias 0.46691811084747314
Batch 400 loss is 0.16274024546146393
At batch 400 layer 0 has weights [0.5625276565551758, 0.043860360980033875] and bias 0.39311012625694275
Batch 500 loss is 0.1624082773923874
At batch 500 layer 0 has weights [0.6029253005981445, 0.06926853209733963] and bias 0.3263569176197052
Batch 600 loss is 0.1860729455947876
At batch 600 layer 0 has weights [0.6429372429847717, 0.09293260425329208] and bias 0.2639945447444916
Batch 700 loss is 0.1350337266921997
At batch 700 layer 0 has weights [0.6816801428794861, 0.11524667590856552] and bias 0.20312638580799103
Batch 800 loss is 0.09294979274272919
At batch 800 layer 0 has weights [0.720126748085022, 0.13712629675865173] and bias 0.14289729297161102
Batch 900 loss is 0.05376340448856354
At batch 900 layer 0 has weights [0.7582015991210938, 0.15822234749794006] and bias 0.08340094983577728
Batch 1000 loss is 0.02638978511095047
At batch 1000 layer 0 has weights [0.7951773405075073, 0.1802600920200348] and bias 0.024842029437422752
Batch 1100 loss is 0.011852355673909187
At batch 1100 layer 0 has weights [0.8165693879127502, 0.184016153216362] and bias 3.638582029452664e-06
Batch 1200 loss is 0.03163360804319382
At batch 1200 layer 0 has weights [0.8252391219139099, 0.1740630567073822] and bias -8.353329030796885e-05
Batch 1300 loss is 0.02630629763007164
At batch 1300 layer 0 has weights [0.835124671459198, 0.16479012370109558] and bias -0.00015536529826931655
Batch 1400 loss is 0.0051133339293301105
At batch 1400 layer 0 has weights [0.844343900680542, 0.15550847351551056] and bias -0.00016727620095480233
Batch 1500 loss is 0.027498461306095123
At batch 1500 layer 0 has weights [0.854489803314209, 0.14521212875843048] and bias -0.00021031628421042114
Batch 1600 loss is 0.004920158535242081
At batch 1600 layer 0 has weights [0.8645113110542297, 0.13410528004169464] and bias 0.00039055143133737147
Batch 1700 loss is 0.008360836654901505
At batch 1700 layer 0 has weights [0.876014232635498, 0.12489280104637146] and bias 0.00015819835243746638
Batch 1800 loss is 0.014614159241318703
At batch 1800 layer 0 has weights [0.8837858438491821, 0.11596223711967468] and bias 1.3087841580272652e-05
Batch 1900 loss is 0.010092692449688911
At batch 1900 layer 0 has weights [0.8932331800460815, 0.10693341493606567] and bias -2.9518936571548693e-05
Batch 2000 loss is 0.009729690849781036
At batch 2000 layer 0 has weights [0.9028034210205078, 0.09779579192399979] and bias 0.0002242704649688676
Batch 2100 loss is 0.016807375475764275
At batch 2100 layer 0 has weights [0.9122253656387329, 0.08806151151657104] and bias 0.00017934542847797275
Batch 2200 loss is 0.00743721891194582
At batch 2200 layer 0 has weights [0.9212979674339294, 0.07864007353782654] and bias 9.568960376782343e-05
Batch 2300 loss is 0.008731607347726822
At batch 2300 layer 0 has weights [0.931233823299408, 0.06903784722089767] and bias -1.799255369405728e-05
Batch 2400 loss is 0.007578771561384201
At batch 2400 layer 0 has weights [0.9399768710136414, 0.05982077494263649] and bias 0.0001134194535552524
Batch 2500 loss is 0.004772713407874107
At batch 2500 layer 0 has weights [0.949438750743866, 0.05062836781144142] and bias -4.91433274873998e-05
Batch 2600 loss is 0.034005217254161835
At batch 2600 layer 0 has weights [0.9602755904197693, 0.04032308980822563] and bias -0.0001682241418166086
Batch 2700 loss is 0.0021029277704656124
At batch 2700 layer 0 has weights [0.9693905711174011, 0.0308067686855793] and bias 6.654067692579702e-05
Batch 2800 loss is 0.004073355346918106
At batch 2800 layer 0 has weights [0.978484570980072, 0.02134108915925026] and bias -1.0443572136864532e-05
Batch 2900 loss is 0.002471046056598425
At batch 2900 layer 0 has weights [0.9884034395217896, 0.01244865171611309] and bias 0.0004622697306331247
Batch 3000 loss is 0.0005060376133769751
At batch 3000 layer 0 has weights [0.997916579246521, 0.002728792140260339] and bias 7.328176434384659e-05
Batch 3100 loss is 7.839938189135864e-05
At batch 3100 layer 0 has weights [1.0001815557479858, -2.829245204338804e-05] and bias -5.204268745728768e-06
Batch 3200 loss is 0.00025801477022469044
At batch 3200 layer 0 has weights [1.0002472400665283, 3.7552331377810333e-06] and bias 0.00014783150982111692
Batch 3300 loss is 0.00011342277139192447
At batch 3300 layer 0 has weights [0.9998117685317993, -4.7483354137511924e-05] and bias 0.00010497608309378847
Batch 3400 loss is 1.2842105206800625e-05
At batch 3400 layer 0 has weights [0.9999505877494812, 5.6136123021133244e-05] and bias -1.3189583114581183e-05
Batch 3500 loss is 8.479270036332309e-05
At batch 3500 layer 0 has weights [1.0000524520874023, 0.00011836915655294433] and bias -5.117881664773449e-05
Batch 3600 loss is 0.00018975354032590985
At batch 3600 layer 0 has weights [0.9998200535774231, -0.00020026051788590848] and bias 0.00010292493971064687
Batch 3700 loss is 6.6317617893219e-05
At batch 3700 layer 0 has weights [0.9999166131019592, 0.00019415473798289895] and bias -4.547740536509082e-05
Batch 3800 loss is 0.03134433925151825
At batch 3800 layer 0 has weights [0.9998785853385925, 2.805422445817385e-05] and bias -6.08101581747178e-05
Batch 3900 loss is 0.00045352234155870974
At batch 3900 layer 0 has weights [0.9997131824493408, -0.0001602886914042756] and bias -0.00011320229532429948
Batch 4000 loss is 0.00016112231242004782
At batch 4000 layer 0 has weights [1.000073790550232, 0.0001504480605944991] and bias 3.487461435724981e-05
Batch 4100 loss is 0.00020764514920301735
At batch 4100 layer 0 has weights [0.9999138116836548, 0.00039887652383185923] and bias 1.1221425666008145e-05
Batch 4200 loss is 7.968989666551352e-05
At batch 4200 layer 0 has weights [0.9998289346694946, 1.5609040929120965e-05] and bias 6.952763214940205e-05
Batch 4300 loss is 9.601350757293403e-05
At batch 4300 layer 0 has weights [1.0002093315124512, -0.0002538618282414973] and bias -4.9954556743614376e-05
Batch 4400 loss is 0.03133075311779976
At batch 4400 layer 0 has weights [0.9999073147773743, 9.930365195032209e-05] and bias -9.861174476100132e-05
Batch 4500 loss is 0.0002791119914036244
At batch 4500 layer 0 has weights [1.0002968311309814, 0.00010068361734738573] and bias 3.98797492380254e-05
Batch 4600 loss is 8.178222924470901e-05
At batch 4600 layer 0 has weights [1.0001376867294312, -5.3197054512565956e-05] and bias -0.0001322573225479573
Batch 4700 loss is 9.00208979146555e-05
At batch 4700 layer 0 has weights [0.9999405741691589, -0.0001393443817505613] and bias 0.00011747400276362896
Batch 4800 loss is 7.315976836252958e-05
At batch 4800 layer 0 has weights [0.9999049305915833, -4.845963849220425e-05] and bias 4.8450285248691216e-05
Batch 4900 loss is 0.0002345034445170313
At batch 4900 layer 0 has weights [1.0000252723693848, -0.0001778743608156219] and bias -0.00013518646301236004
Batch 5000 loss is 3.982471753261052e-05
At batch 5000 layer 0 has weights [1.0001646280288696, -7.575102245027665e-06] and bias 8.62662200233899e-05
