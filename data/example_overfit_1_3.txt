Batch 0 loss is 0.5572036504745483
At batch 0 layer 0 has weights [0.3906121850013733, -0.48973608016967773] and bias 0.42729514837265015
Batch 100 loss is 0.44421103596687317
At batch 100 layer 0 has weights [0.4888319671154022, -0.3926273286342621] and bias 0.4211511015892029
Batch 200 loss is 0.3478423058986664
At batch 200 layer 0 has weights [0.5870871543884277, -0.2946300506591797] and bias 0.40622037649154663
Batch 300 loss is 0.27962762117385864
At batch 300 layer 0 has weights [0.6749756932258606, -0.1927022486925125] and bias 0.3684055805206299
Batch 400 loss is 0.17322495579719543
At batch 400 layer 0 has weights [0.7614482045173645, -0.09385014325380325] and bias 0.3135727643966675
Batch 500 loss is 0.11096925288438797
At batch 500 layer 0 has weights [0.8180553317070007, -0.027049867436289787] and bias 0.20804907381534576
Batch 600 loss is 0.060726165771484375
At batch 600 layer 0 has weights [0.865813672542572, 0.020966671407222748] and bias 0.11198761314153671
Batch 700 loss is 0.014921722002327442
At batch 700 layer 0 has weights [0.9122419357299805, 0.056302860379219055] and bias 0.031415123492479324
Batch 800 loss is 0.0023958799429237843
At batch 800 layer 0 has weights [0.9334481358528137, 0.06684038043022156] and bias 0.00018335679487790912
Batch 900 loss is 0.007846331223845482
At batch 900 layer 0 has weights [0.9386864900588989, 0.061491094529628754] and bias 0.0001318430295214057
Batch 1000 loss is 0.003819892415776849
At batch 1000 layer 0 has weights [0.9436438083648682, 0.05649213120341301] and bias -0.00035708039649762213
Batch 1100 loss is 0.006741974037140608
At batch 1100 layer 0 has weights [0.948544442653656, 0.051425378769636154] and bias 0.00042845902498811483
Batch 1200 loss is 0.0015634367009624839
At batch 1200 layer 0 has weights [0.9542319178581238, 0.04602585732936859] and bias -0.00016361729649361223
Batch 1300 loss is 0.0029200385324656963
At batch 1300 layer 0 has weights [0.9594983458518982, 0.040633201599121094] and bias 0.00037368814810179174
Batch 1400 loss is 0.002201822819188237
At batch 1400 layer 0 has weights [0.9654573202133179, 0.03445475175976753] and bias 5.8204786910209805e-05
Batch 1500 loss is 0.0003115482977591455
At batch 1500 layer 0 has weights [0.9717791676521301, 0.02868291363120079] and bias 0.0001527219283161685
Batch 1600 loss is 0.001666773110628128
At batch 1600 layer 0 has weights [0.9771689176559448, 0.022961972281336784] and bias 0.00020867575949523598
Batch 1700 loss is 0.0012093631085008383
At batch 1700 layer 0 has weights [0.9826765656471252, 0.017229998484253883] and bias -7.698102854192257e-05
Batch 1800 loss is 0.0009831121424213052
At batch 1800 layer 0 has weights [0.9882522225379944, 0.011691983789205551] and bias 0.00031232216861099005
Batch 1900 loss is 0.0008283167844638228
At batch 1900 layer 0 has weights [0.9938469529151917, 0.006021555513143539] and bias 0.00036512542283162475
Batch 2000 loss is 0.00017339539772365242
At batch 2000 layer 0 has weights [0.9991612434387207, 0.0007174877100624144] and bias 0.00015204751980490983
Batch 2100 loss is 8.930204785428941e-05
At batch 2100 layer 0 has weights [0.9998769164085388, 0.0001920760696521029] and bias -0.00010101759107783437
Batch 2200 loss is 6.054675031919032e-05
At batch 2200 layer 0 has weights [0.9999346137046814, -6.391796341631562e-05] and bias 8.0507954407949e-05
Batch 2300 loss is 0.00023422169033437967
At batch 2300 layer 0 has weights [0.9997296333312988, -5.943661108176457e-06] and bias -4.5173215767135844e-05
Batch 2400 loss is 3.5483612009556964e-05
At batch 2400 layer 0 has weights [0.9999525547027588, 0.00010737476986832917] and bias -3.0404296467168024e-06
Batch 2500 loss is 8.74899560585618e-05
At batch 2500 layer 0 has weights [1.0000033378601074, -0.00019262816931586713] and bias -4.77822595712496e-06
Batch 2600 loss is 0.0002168873616028577
At batch 2600 layer 0 has weights [0.9999470710754395, 0.0002295636513736099] and bias 0.0001467880792915821
Batch 2700 loss is 0.0003896459238603711
At batch 2700 layer 0 has weights [1.0003081560134888, 0.00032901152735576034] and bias 0.00015202352369669825
Batch 2800 loss is 0.0003177173202857375
At batch 2800 layer 0 has weights [1.000097393989563, -1.6354450053768232e-05] and bias 0.0002786874829325825
Batch 2900 loss is 0.00021334929624572396
At batch 2900 layer 0 has weights [1.0000672340393066, 0.00015612822608090937] and bias 0.0001295716647291556
Batch 3000 loss is 0.0005959022673778236
At batch 3000 layer 0 has weights [0.9999917149543762, -0.0001700106804491952] and bias -0.0004895308520644903
Batch 3100 loss is 5.711382254958153e-05
At batch 3100 layer 0 has weights [0.9999723434448242, 0.00014002273383084685] and bias -5.7286575611215085e-05
Batch 3200 loss is 0.0005905962898395956
At batch 3200 layer 0 has weights [0.9995671510696411, -0.00037795345997437835] and bias -0.00017167709302157164
Batch 3300 loss is 0.00018697051564231515
At batch 3300 layer 0 has weights [0.9997486472129822, -0.00015662130317650735] and bias 0.0002720903721638024
Batch 3400 loss is 7.3051436629612e-05
At batch 3400 layer 0 has weights [0.999924898147583, 3.295591159258038e-05] and bias -5.35852050234098e-05
Batch 3500 loss is 0.00010269724589306861
At batch 3500 layer 0 has weights [0.9998582601547241, -0.00010058456973638386] and bias 5.987633630866185e-05
Batch 3600 loss is 0.00017931940965354443
At batch 3600 layer 0 has weights [0.9999918341636658, 0.00016945265815593302] and bias -0.00024936115369200706
Batch 3700 loss is 0.0001643143768887967
At batch 3700 layer 0 has weights [1.000103235244751, 3.9115515392040834e-05] and bias 0.00011215094127692282
Batch 3800 loss is 0.000208173762075603
At batch 3800 layer 0 has weights [1.0001939535140991, 0.00020490917086135596] and bias -2.6900614102487452e-05
Batch 3900 loss is 4.848446405958384e-05
At batch 3900 layer 0 has weights [1.0001471042633057, -0.00022224073472898453] and bias 7.913969602668658e-05
Batch 4000 loss is 4.927979171043262e-05
At batch 4000 layer 0 has weights [0.9999240636825562, 0.00018934253603219986] and bias -0.00010901632776949555
Batch 4100 loss is 9.746133582666516e-05
At batch 4100 layer 0 has weights [1.000076174736023, -0.0002138295240001753] and bias -3.261178426328115e-05
Batch 4200 loss is 0.00018533936236053705
At batch 4200 layer 0 has weights [1.0000951290130615, -9.874755778582767e-05] and bias -0.00019301411521155387
Batch 4300 loss is 0.00010152019967790693
At batch 4300 layer 0 has weights [0.9999955296516418, -7.530152652179822e-05] and bias 0.00013892525748815387
Batch 4400 loss is 0.00013614303315989673
At batch 4400 layer 0 has weights [0.9998915791511536, -0.00022597392671741545] and bias -1.1040877325285692e-05
Batch 4500 loss is 0.00019501789938658476
At batch 4500 layer 0 has weights [0.9998078942298889, -0.0002550618664827198] and bias 0.000122657889733091
Batch 4600 loss is 0.00020808761473745108
At batch 4600 layer 0 has weights [0.9998718500137329, -1.7914224372361787e-05] and bias -0.00014073072816245258
Batch 4700 loss is 5.5556942243129015e-05
At batch 4700 layer 0 has weights [0.9999195337295532, 0.00017572732758708298] and bias 2.5796341560635483e-06
Batch 4800 loss is 0.00010552737512625754
At batch 4800 layer 0 has weights [0.9998105764389038, -4.497042755247094e-05] and bias 7.060341886244714e-05
Batch 4900 loss is 0.00019446844817139208
At batch 4900 layer 0 has weights [0.99982750415802, -5.586594852502458e-06] and bias -0.0001002131393761374
Batch 5000 loss is 0.0005433628102764487
At batch 5000 layer 0 has weights [1.000075101852417, -7.000137429713504e-06] and bias -0.00044123618863523006
