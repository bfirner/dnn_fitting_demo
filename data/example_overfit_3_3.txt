Batch 0 loss is 0.5480587482452393
At batch 0 layer 0 has weights [0.3010328412055969, -0.48452118039131165] and bias 0.47171109914779663
Batch 100 loss is 0.4338987469673157
At batch 100 layer 0 has weights [0.39891910552978516, -0.3938770890235901] and bias 0.4920470416545868
Batch 200 loss is 0.39422136545181274
At batch 200 layer 0 has weights [0.49790307879447937, -0.30047380924224854] and bias 0.49501028656959534
Batch 300 loss is 0.30385950207710266
At batch 300 layer 0 has weights [0.596502423286438, -0.207499161362648] and bias 0.4998866319656372
Batch 400 loss is 0.23405805230140686
At batch 400 layer 0 has weights [0.6779800057411194, -0.134576678276062] and bias 0.4550306797027588
Batch 500 loss is 0.13152071833610535
At batch 500 layer 0 has weights [0.7284131646156311, -0.09671387076377869] and bias 0.3681277632713318
Batch 600 loss is 0.1350673884153366
At batch 600 layer 0 has weights [0.7718915343284607, -0.06905345618724823] and bias 0.2957448363304138
Batch 700 loss is 0.12069057673215866
At batch 700 layer 0 has weights [0.815988302230835, -0.042373377829790115] and bias 0.22614355385303497
Batch 800 loss is 0.09676553308963776
At batch 800 layer 0 has weights [0.8577921390533447, -0.017730839550495148] and bias 0.15823587775230408
Batch 900 loss is 0.05347235128283501
At batch 900 layer 0 has weights [0.9000149965286255, 0.0054721045307815075] and bias 0.09482172131538391
Batch 1000 loss is 0.022236213088035583
At batch 1000 layer 0 has weights [0.9400250911712646, 0.028500346466898918] and bias 0.031045325100421906
Batch 1100 loss is 0.034852445125579834
At batch 1100 layer 0 has weights [0.9638714790344238, 0.03565564379096031] and bias -7.043439109111205e-05
Batch 1200 loss is 0.003329491475597024
At batch 1200 layer 0 has weights [0.9741086363792419, 0.02571297623217106] and bias -4.23423589381855e-05
Batch 1300 loss is 0.0018313771579414606
At batch 1300 layer 0 has weights [0.9841997623443604, 0.015982449054718018] and bias 0.00025334040401503444
Batch 1400 loss is 0.000963655998930335
At batch 1400 layer 0 has weights [0.9943939447402954, 0.006399947218596935] and bias -6.8106317485217e-05
Batch 1500 loss is 0.03144311532378197
At batch 1500 layer 0 has weights [0.9998741745948792, 0.0002673683047760278] and bias -0.0002795875770971179
Batch 1600 loss is 0.0013890434056520462
At batch 1600 layer 0 has weights [0.999635636806488, -0.0002472689375281334] and bias -0.0009225946851074696
Batch 1700 loss is 6.116385338827968e-05
At batch 1700 layer 0 has weights [0.9999767541885376, 7.132410974008963e-05] and bias -9.636674076318741e-05
Batch 1800 loss is 0.0002141809236491099
At batch 1800 layer 0 has weights [0.9998959898948669, 1.087302098312648e-05] and bias -0.00016861694166436791
Batch 1900 loss is 0.00014517418458126485
At batch 1900 layer 0 has weights [0.999876081943512, 0.00042673092684708536] and bias -0.00022091492428444326
Batch 2000 loss is 0.031345661729574203
At batch 2000 layer 0 has weights [0.9997789859771729, 0.00010377153375884518] and bias 0.0001997870422201231
Batch 2100 loss is 0.000169166291016154
At batch 2100 layer 0 has weights [0.9999588131904602, -5.618423529085703e-05] and bias 0.00023448529827874154
Batch 2200 loss is 4.279137647245079e-05
At batch 2200 layer 0 has weights [0.9999492168426514, 0.0001268247578991577] and bias -4.5786204282194376e-05
Batch 2300 loss is 0.0003759174724109471
At batch 2300 layer 0 has weights [1.0000747442245483, 0.00011967510363319889] and bias 0.0002553109952714294
Batch 2400 loss is 0.0005996113759465516
At batch 2400 layer 0 has weights [1.0005513429641724, 0.0003339762915857136] and bias 0.00015326926950365305
Batch 2500 loss is 0.00021496284171007574
At batch 2500 layer 0 has weights [0.9999478459358215, 0.0004756028938572854] and bias -2.710771241254406e-06
Batch 2600 loss is 0.00038789608515799046
At batch 2600 layer 0 has weights [1.0000498294830322, 8.772060391493142e-05] and bias 0.0003136199957225472
Batch 2700 loss is 0.0003823496517725289
At batch 2700 layer 0 has weights [0.9998841881752014, 0.0003204883250873536] and bias 0.00022990963771007955
Batch 2800 loss is 0.00016406323993578553
At batch 2800 layer 0 has weights [1.000063419342041, 0.0002545529860071838] and bias -8.672178228152916e-05
Batch 2900 loss is 0.00015424835146404803
At batch 2900 layer 0 has weights [0.9999150633811951, 0.0002079089463222772] and bias -0.00023524407879449427
Batch 3000 loss is 6.599978951271623e-05
At batch 3000 layer 0 has weights [0.9998826384544373, 7.5864572863793e-06] and bias 8.261076436610892e-05
Batch 3100 loss is 0.00013542603119276464
At batch 3100 layer 0 has weights [0.9997205138206482, 0.00018475798424333334] and bias -9.496950951870531e-05
Batch 3200 loss is 3.040340743609704e-05
At batch 3200 layer 0 has weights [1.0000752210617065, -2.4872415451682173e-05] and bias -8.129837624437641e-06
Batch 3300 loss is 0.0002444865822326392
At batch 3300 layer 0 has weights [1.000226378440857, 0.00025158439530059695] and bias 3.380237103556283e-05
Batch 3400 loss is 5.911328480578959e-05
At batch 3400 layer 0 has weights [1.0001485347747803, -2.9795930458931252e-05] and bias -7.518446363974363e-05
Batch 3500 loss is 0.00016302004223689437
At batch 3500 layer 0 has weights [0.9997385144233704, -8.447346044704318e-05] and bias 6.880375440232456e-05
Batch 3600 loss is 9.24883788684383e-05
At batch 3600 layer 0 has weights [0.9999483227729797, 0.0002327942638657987] and bias -0.0001119031585403718
Batch 3700 loss is 8.903224806999788e-05
At batch 3700 layer 0 has weights [0.999997079372406, -8.411446469835937e-05] and bias -1.596076981513761e-05
Batch 3800 loss is 0.00010095415927935392
At batch 3800 layer 0 has weights [0.999878466129303, -0.00011687866935972124] and bias 0.00014896632637828588
Batch 3900 loss is 0.0006257002241909504
At batch 3900 layer 0 has weights [1.0001589059829712, 0.00024434426450170577] and bias 0.00041373344720341265
Batch 4000 loss is 0.031261276453733444
At batch 4000 layer 0 has weights [1.0000207424163818, -1.8682025256566703e-05] and bias 1.1316431482555345e-05
Batch 4100 loss is 0.0002100198034895584
At batch 4100 layer 0 has weights [1.0001698732376099, 0.00012589730613399297] and bias 8.592267113272101e-05
Batch 4200 loss is 0.00015648567932657897
At batch 4200 layer 0 has weights [1.0001384019851685, 0.00021934177493676543] and bias -0.00019513563893269747
Batch 4300 loss is 0.0002418413496343419
At batch 4300 layer 0 has weights [1.0001498460769653, -4.3856540287379175e-05] and bias 0.00018636226013768464
Batch 4400 loss is 0.03130703791975975
At batch 4400 layer 0 has weights [0.9999844431877136, 0.00010753241804195568] and bias 7.124079729692312e-06
Batch 4500 loss is 3.069744707318023e-05
At batch 4500 layer 0 has weights [1.0000027418136597, -2.773725282168016e-05] and bias -1.6389540178352036e-05
Batch 4600 loss is 2.4264656531158835e-05
At batch 4600 layer 0 has weights [0.9999659061431885, 8.931733464123681e-05] and bias -5.423591937869787e-05
Batch 4700 loss is 0.0003814492956735194
At batch 4700 layer 0 has weights [0.9994350075721741, -0.00011389132123440504] and bias -5.6107175623765215e-05
Batch 4800 loss is 0.0002055629447568208
At batch 4800 layer 0 has weights [1.000196099281311, 0.0001881349307950586] and bias 3.20996914524585e-05
Batch 4900 loss is 0.0005169940995983779
At batch 4900 layer 0 has weights [1.0000554323196411, 0.000512672879267484] and bias 0.0001866123202489689
Batch 5000 loss is 0.0006303749978542328
At batch 5000 layer 0 has weights [1.0000708103179932, 0.00042639015009626746] and bias 8.697853627381846e-05
