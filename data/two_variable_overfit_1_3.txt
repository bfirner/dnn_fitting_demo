Batch 0 loss is 1.0152274370193481
At batch 0 layer 0 has weights [-0.5367830991744995, -0.04131269454956055] and bias -0.17686402797698975
Batch 100 loss is 0.7673152685165405
At batch 100 layer 0 has weights [-0.4381866455078125, 0.057283662259578705] and bias -0.07686382532119751
Batch 200 loss is 0.6254774332046509
At batch 200 layer 0 has weights [-0.33689266443252563, 0.15857776999473572] and bias 0.008038113825023174
Batch 300 loss is 0.5780318379402161
At batch 300 layer 0 has weights [-0.23636458814144135, 0.2591058015823364] and bias 0.011816268786787987
Batch 400 loss is 0.3195164203643799
At batch 400 layer 0 has weights [-0.1381247490644455, 0.3573457598686218] and bias 0.012399839237332344
Batch 500 loss is 0.2585906982421875
At batch 500 layer 0 has weights [-0.042038194835186005, 0.4534323811531067] and bias 0.00860525667667389
Batch 600 loss is 0.2053907811641693
At batch 600 layer 0 has weights [0.05823546648025513, 0.553706169128418] and bias 0.012243511155247688
Batch 700 loss is 0.08002706617116928
At batch 700 layer 0 has weights [0.1568685621023178, 0.6523392796516418] and bias 0.01342807151377201
Batch 800 loss is 0.0040664756670594215
At batch 800 layer 0 has weights [0.2506575882434845, 0.746128261089325] and bias 0.0058744484558701515
Batch 900 loss is 0.00012687193520832807
At batch 900 layer 0 has weights [0.25217947363853455, 0.7476503252983093] and bias -5.2398616389837116e-05
Batch 1000 loss is 0.00022795709082856774
At batch 1000 layer 0 has weights [0.25230392813682556, 0.7477750778198242] and bias 0.00019339566642884165
Batch 1100 loss is 0.00021337674115784466
At batch 1100 layer 0 has weights [0.25239765644073486, 0.7478687167167664] and bias -0.00034652563044801354
Batch 1200 loss is 8.550276106689125e-05
At batch 1200 layer 0 has weights [0.2521727681159973, 0.7476437091827393] and bias 0.00011674536654027179
Batch 1300 loss is 0.00016785485786385834
At batch 1300 layer 0 has weights [0.25217586755752563, 0.7476467490196228] and bias 0.000234378530876711
Batch 1400 loss is 0.00035619898699223995
At batch 1400 layer 0 has weights [0.2523878216743469, 0.747858464717865] and bias -0.00046393898082897067
Batch 1500 loss is 0.00022854522103443742
At batch 1500 layer 0 has weights [0.2524928152561188, 0.747963547706604] and bias -0.00023125039297156036
Batch 1600 loss is 0.00026614556554704905
At batch 1600 layer 0 has weights [0.25198131799697876, 0.7474517226219177] and bias 0.00014487844600807875
Batch 1700 loss is 0.0004628098104149103
At batch 1700 layer 0 has weights [0.2522988021373749, 0.7477691769599915] and bias 0.00042035742080770433
Batch 1800 loss is 0.0003462271415628493
At batch 1800 layer 0 has weights [0.25212201476097107, 0.7475923299789429] and bias -0.0001944870746228844
Batch 1900 loss is 6.252058665268123e-05
At batch 1900 layer 0 has weights [0.25224068760871887, 0.7477112412452698] and bias -4.000693297712132e-05
Batch 2000 loss is 0.0006607369286939502
At batch 2000 layer 0 has weights [0.25239288806915283, 0.747863233089447] and bias 0.0005487019079737365
Batch 2100 loss is 0.00010133223258890212
At batch 2100 layer 0 has weights [0.2521488666534424, 0.7476195096969604] and bias 0.000173759093740955
Batch 2200 loss is 0.00027975570992566645
At batch 2200 layer 0 has weights [0.25221502780914307, 0.7476855516433716] and bias -0.00023936187790241092
Batch 2300 loss is 0.0003341658157296479
At batch 2300 layer 0 has weights [0.2524208426475525, 0.7478914260864258] and bias 0.0001975162304006517
Batch 2400 loss is 9.21037863008678e-05
At batch 2400 layer 0 has weights [0.252172589302063, 0.747643232345581] and bias 7.483305671485141e-05
Batch 2500 loss is 0.0001330191153101623
At batch 2500 layer 0 has weights [0.25224190950393677, 0.7477125525474548] and bias -0.00011878688383148983
Batch 2600 loss is 9.149641846306622e-05
At batch 2600 layer 0 has weights [0.2521725594997406, 0.7476430535316467] and bias 8.092827192740515e-05
Batch 2700 loss is 0.00015423260629177094
At batch 2700 layer 0 has weights [0.2522832751274109, 0.747753918170929] and bias 0.0001298351853620261
Batch 2800 loss is 0.0002564036403782666
At batch 2800 layer 0 has weights [0.2521967887878418, 0.7476673126220703] and bias -0.0001969590812223032
Batch 2900 loss is 0.00013636195217259228
At batch 2900 layer 0 has weights [0.25209513306617737, 0.7475656270980835] and bias 8.092351345112547e-05
Batch 3000 loss is 0.00015712450840510428
At batch 3000 layer 0 has weights [0.25223293900489807, 0.7477031946182251] and bias -0.00012917511048726737
Batch 3100 loss is 0.00018271320732310414
At batch 3100 layer 0 has weights [0.25206559896469116, 0.7475355863571167] and bias -3.31681985699106e-05
Batch 3200 loss is 0.00024297050549648702
At batch 3200 layer 0 has weights [0.25233033299446106, 0.7478002905845642] and bias 0.0001776657736627385
Batch 3300 loss is 0.0003140754997730255
At batch 3300 layer 0 has weights [0.25237348675727844, 0.7478434443473816] and bias 0.00022595807968173176
Batch 3400 loss is 6.550068792421371e-05
At batch 3400 layer 0 has weights [0.2522211968898773, 0.7476912140846252] and bias -1.8951941456180066e-05
Batch 3500 loss is 0.00047850297414697707
At batch 3500 layer 0 has weights [0.2518271207809448, 0.7472969889640808] and bias -9.529565431876108e-05
Batch 3600 loss is 0.00026987711316905916
At batch 3600 layer 0 has weights [0.2522999942302704, 0.7477699518203735] and bias 0.0002370482834521681
Batch 3700 loss is 0.0001853683206718415
At batch 3700 layer 0 has weights [0.25207993388175964, 0.7475493550300598] and bias 0.00034629873698577285
Batch 3800 loss is 0.0006148273241706192
At batch 3800 layer 0 has weights [0.2525211274623871, 0.7479905486106873] and bias 0.0003909631632268429
Batch 3900 loss is 0.00015539382002316415
At batch 3900 layer 0 has weights [0.2523352801799774, 0.747804582118988] and bias -0.00022531457943841815
Batch 4000 loss is 0.0003534253337420523
At batch 4000 layer 0 has weights [0.25256723165512085, 0.7480367422103882] and bias 0.00012695723853539675
Batch 4100 loss is 5.679109744960442e-05
At batch 4100 layer 0 has weights [0.2523193061351776, 0.747789204120636] and bias -1.3881871382181998e-05
Batch 4200 loss is 0.00019778896239586174
At batch 4200 layer 0 has weights [0.2523675560951233, 0.7478378415107727] and bias 8.864855044521391e-05
Batch 4300 loss is 0.0004368614172562957
At batch 4300 layer 0 has weights [0.2525460422039032, 0.7480164170265198] and bias 0.00013810070231556892
Batch 4400 loss is 5.9875477745663375e-05
At batch 4400 layer 0 has weights [0.2522737979888916, 0.7477442026138306] and bias 4.860462649958208e-05
Batch 4500 loss is 0.00011144061863888055
At batch 4500 layer 0 has weights [0.2522956430912018, 0.7477660775184631] and bias -0.00014424811524804682
Batch 4600 loss is 0.00025390111841261387
At batch 4600 layer 0 has weights [0.2522551417350769, 0.7477253675460815] and bias -0.00024292348825838417
Batch 4700 loss is 0.0001888524420792237
At batch 4700 layer 0 has weights [0.25234460830688477, 0.7478150129318237] and bias 9.906879859045148e-05
Batch 4800 loss is 0.0002614876430016011
At batch 4800 layer 0 has weights [0.2522285580635071, 0.7476988434791565] and bias 0.00028643468976952136
Batch 4900 loss is 0.00048619581502862275
At batch 4900 layer 0 has weights [0.2519806921482086, 0.7474510073661804] and bias -0.00021981625468470156
Batch 5000 loss is 0.0001859281910583377
At batch 5000 layer 0 has weights [0.25223681330680847, 0.7477067708969116] and bias -0.00015401335258502513
