Batch 0 loss is 0.23863929510116577
At batch 0 layer 0 has weights [0.1801549196243286, 0.5015397667884827] and bias -0.04391944408416748
Batch 100 loss is 0.13601171970367432
At batch 100 layer 0 has weights [0.2781003415584564, 0.5998349189758301] and bias 0.011963838711380959
Batch 200 loss is 0.062312446534633636
At batch 200 layer 0 has weights [0.3408193588256836, 0.6578371524810791] and bias 0.00016725460591260344
Batch 300 loss is 0.04098027944564819
At batch 300 layer 0 has weights [0.3470715880393982, 0.6532090306282043] and bias 2.6521342078922316e-05
Batch 400 loss is 0.061080452054739
At batch 400 layer 0 has weights [0.35212674736976624, 0.6477178335189819] and bias -0.00026946174330078065
Batch 500 loss is 0.04062136635184288
At batch 500 layer 0 has weights [0.35768234729766846, 0.6421656012535095] and bias -0.00040049338713288307
Batch 600 loss is 0.020205030217766762
At batch 600 layer 0 has weights [0.36280861496925354, 0.6369950175285339] and bias -0.00020691023382823914
Batch 700 loss is 0.03970612958073616
At batch 700 layer 0 has weights [0.36838480830192566, 0.6319549083709717] and bias 2.040203798969742e-05
Batch 800 loss is 0.03942463546991348
At batch 800 layer 0 has weights [0.37324875593185425, 0.6264619827270508] and bias -7.190802716650069e-05
Batch 900 loss is 0.019672930240631104
At batch 900 layer 0 has weights [0.37883734703063965, 0.6216196417808533] and bias 3.527192529872991e-05
Batch 1000 loss is 0.019513027742505074
At batch 1000 layer 0 has weights [0.38360893726348877, 0.6158362030982971] and bias 0.00024031131761148572
Batch 1100 loss is 0.000747589161619544
At batch 1100 layer 0 has weights [0.3887368142604828, 0.6104929447174072] and bias -0.0003143330686725676
Batch 1200 loss is 0.00037428055657073855
At batch 1200 layer 0 has weights [0.39425531029701233, 0.605381190776825] and bias -0.00018117571016773582
Batch 1300 loss is 0.00028786167968064547
At batch 1300 layer 0 has weights [0.3998488187789917, 0.600472092628479] and bias 0.00012737720680888742
Batch 1400 loss is 0.018904412165284157
At batch 1400 layer 0 has weights [0.40460526943206787, 0.5957357287406921] and bias -0.0004261852300260216
Batch 1500 loss is 0.03742415830492973
At batch 1500 layer 0 has weights [0.41012075543403625, 0.5901008248329163] and bias -0.0006328816525638103
Batch 1600 loss is 0.018297884613275528
At batch 1600 layer 0 has weights [0.41548818349838257, 0.5845775604248047] and bias -2.752504587988369e-05
Batch 1700 loss is 0.03646225482225418
At batch 1700 layer 0 has weights [0.42111438512802124, 0.579095184803009] and bias -0.00037358468398451805
Batch 1800 loss is 0.036027275025844574
At batch 1800 layer 0 has weights [0.4261988699436188, 0.5741419196128845] and bias -0.00012490100925788283
Batch 1900 loss is 0.05344872176647186
At batch 1900 layer 0 has weights [0.4312905967235565, 0.5683980584144592] and bias 0.00018620416813064367
Batch 2000 loss is 0.03527049347758293
At batch 2000 layer 0 has weights [0.43692490458488464, 0.5632333159446716] and bias -9.137624874711037e-05
Batch 2100 loss is 4.171130058239214e-05
At batch 2100 layer 0 has weights [0.4425656199455261, 0.5574783682823181] and bias 2.1084562831674702e-05
Batch 2200 loss is 0.017493389546871185
At batch 2200 layer 0 has weights [0.44813358783721924, 0.552141547203064] and bias 0.0001265157770831138
Batch 2300 loss is 0.017374321818351746
At batch 2300 layer 0 has weights [0.45380380749702454, 0.5465660691261292] and bias 0.00014109571930021048
Batch 2400 loss is 0.0339866504073143
At batch 2400 layer 0 has weights [0.4587860703468323, 0.5413690209388733] and bias -0.00022862167679704726
Batch 2500 loss is 0.01720312424004078
At batch 2500 layer 0 has weights [0.4646831154823303, 0.5359289050102234] and bias 0.00013891012349631637
Batch 2600 loss is 0.03349056839942932
At batch 2600 layer 0 has weights [0.46887123584747314, 0.5316583514213562] and bias 3.4594995668157935e-05
Batch 2700 loss is 0.0003560563491191715
At batch 2700 layer 0 has weights [0.47373953461647034, 0.525628924369812] and bias -5.9994901675963774e-05
Batch 2800 loss is 0.03267433121800423
At batch 2800 layer 0 has weights [0.48002517223358154, 0.5198830962181091] and bias -0.0001472532021580264
Batch 2900 loss is 0.06442110240459442
At batch 2900 layer 0 has weights [0.4854767620563507, 0.5142343044281006] and bias 0.00016575379413552582
Batch 3000 loss is 0.03187544271349907
At batch 3000 layer 0 has weights [0.49112996459007263, 0.5089898705482483] and bias -0.00011944671132368967
Batch 3100 loss is 0.01578737050294876
At batch 3100 layer 0 has weights [0.49662089347839355, 0.5032382607460022] and bias 2.113214577548206e-05
Batch 3200 loss is 0.015791164711117744
At batch 3200 layer 0 has weights [0.5021460056304932, 0.498275488615036] and bias 3.799926707870327e-05
Batch 3300 loss is 0.07704927772283554
At batch 3300 layer 0 has weights [0.5070093870162964, 0.4928986728191376] and bias 8.460889512207359e-05
Batch 3400 loss is 0.00019684679864440113
At batch 3400 layer 0 has weights [0.5134544372558594, 0.4867512285709381] and bias 8.117013203445822e-05
Batch 3500 loss is 0.030125459656119347
At batch 3500 layer 0 has weights [0.5185725092887878, 0.48149943351745605] and bias -3.703336915350519e-05
Batch 3600 loss is 0.014961385168135166
At batch 3600 layer 0 has weights [0.5246323347091675, 0.47541797161102295] and bias 8.972905925475061e-05
Batch 3700 loss is 0.044124484062194824
At batch 3700 layer 0 has weights [0.5304933786392212, 0.4692401885986328] and bias 6.782281707273796e-05
Batch 3800 loss is 0.00021789385937154293
At batch 3800 layer 0 has weights [0.5367739200592041, 0.46345242857933044] and bias 0.00013298056728672236
Batch 3900 loss is 0.028820496052503586
At batch 3900 layer 0 has weights [0.5423114895820618, 0.4580824077129364] and bias 6.278463115449995e-05
Batch 4000 loss is 0.014425491914153099
At batch 4000 layer 0 has weights [0.5481630563735962, 0.45225828886032104] and bias 0.0001433997240383178
Batch 4100 loss is 0.013989577069878578
At batch 4100 layer 0 has weights [0.5531219840049744, 0.4468647539615631] and bias 3.248354551033117e-05
Batch 4200 loss is 0.05537433922290802
At batch 4200 layer 0 has weights [0.5584803819656372, 0.441993772983551] and bias -6.8352346715983e-05
Batch 4300 loss is 0.013775485567748547
At batch 4300 layer 0 has weights [0.5635865926742554, 0.43669500946998596] and bias 3.4117791074095294e-05
Batch 4400 loss is 0.013644658960402012
At batch 4400 layer 0 has weights [0.5685274004936218, 0.43162214756011963] and bias -0.00024054721870925277
Batch 4500 loss is 0.014075464569032192
At batch 4500 layer 0 has weights [0.5731964707374573, 0.42616280913352966] and bias -0.00043750731856562197
Batch 4600 loss is 6.628109986195341e-05
At batch 4600 layer 0 has weights [0.579388439655304, 0.4207088351249695] and bias 1.1564987289602868e-05
Batch 4700 loss is 0.00019038881873711944
At batch 4700 layer 0 has weights [0.5842646360397339, 0.41554078459739685] and bias -7.489196286769584e-05
Batch 4800 loss is 0.01303941197693348
At batch 4800 layer 0 has weights [0.5892907977104187, 0.41066834330558777] and bias 0.0002402071258984506
Batch 4900 loss is 0.050855934619903564
At batch 4900 layer 0 has weights [0.5944348573684692, 0.40505483746528625] and bias 8.939630788518116e-05
Batch 5000 loss is 0.037780020385980606
At batch 5000 layer 0 has weights [0.600229024887085, 0.39922502636909485] and bias 0.0002147870691260323
