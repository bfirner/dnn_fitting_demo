Batch 0 loss is 0.5175698399543762
At batch 0 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 0 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 0 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.2188827395439148
Batch 100 loss is 0.5852750539779663
At batch 100 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 100 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 100 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.22711986303329468
Batch 200 loss is 0.5482579469680786
At batch 200 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 200 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 200 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.24262404441833496
Batch 300 loss is 0.4346967339515686
At batch 300 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 300 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 300 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.23878690600395203
Batch 400 loss is 0.46526336669921875
At batch 400 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 400 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 400 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.22210702300071716
Batch 500 loss is 0.5
At batch 500 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 500 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 500 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.20994411408901215
Batch 600 loss is 0.5
At batch 600 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 600 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 600 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.20508980751037598
Batch 700 loss is 0.4812626242637634
At batch 700 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 700 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 700 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.20020242035388947
Batch 800 loss is 0.5188531279563904
At batch 800 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 800 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 800 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.19835013151168823
Batch 900 loss is 0.4613664448261261
At batch 900 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 900 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 900 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.19093167781829834
Batch 1000 loss is 0.5404216051101685
At batch 1000 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 1000 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 1000 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.1766272783279419
Batch 1100 loss is 0.541847825050354
At batch 1100 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 1100 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 1100 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.16521719098091125
Batch 1200 loss is 0.6065926551818848
At batch 1200 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 1200 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 1200 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.158903568983078
Batch 1300 loss is 0.5209517478942871
At batch 1300 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 1300 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 1300 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.16477173566818237
Batch 1400 loss is 0.47899556159973145
At batch 1400 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 1400 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 1400 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.16392861306667328
Batch 1500 loss is 0.4588700234889984
At batch 1500 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 1500 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 1500 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.1709602177143097
Batch 1600 loss is 0.4145212769508362
At batch 1600 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 1600 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 1600 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.15808513760566711
Batch 1700 loss is 0.5410745143890381
At batch 1700 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 1700 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 1700 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.17140410840511322
Batch 1800 loss is 0.5419489741325378
At batch 1800 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 1800 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 1800 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.1644081473350525
Batch 1900 loss is 0.4574505090713501
At batch 1900 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 1900 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 1900 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.15960407257080078
Batch 2000 loss is 0.4778704047203064
At batch 2000 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 2000 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 2000 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.1459268182516098
Batch 2100 loss is 0.4547452926635742
At batch 2100 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 2100 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 2100 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.13796235620975494
Batch 2200 loss is 0.5220862030982971
At batch 2200 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 2200 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 2200 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.14662082493305206
Batch 2300 loss is 0.4776844382286072
At batch 2300 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 2300 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 2300 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.1429513841867447
Batch 2400 loss is 0.6346593499183655
At batch 2400 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 2400 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 2400 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.1409083604812622
Batch 2500 loss is 0.5221399068832397
At batch 2500 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 2500 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 2500 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.14576157927513123
Batch 2600 loss is 0.523165762424469
At batch 2600 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 2600 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 2600 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.12934768199920654
Batch 2700 loss is 0.5227567553520203
At batch 2700 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 2700 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 2700 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.13589203357696533
Batch 2800 loss is 0.5232715010643005
At batch 2800 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 2800 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 2800 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.127656027674675
Batch 2900 loss is 0.4324479401111603
At batch 2900 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 2900 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 2900 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.13972234725952148
Batch 3000 loss is 0.5454492568969727
At batch 3000 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 3000 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 3000 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.13640575110912323
Batch 3100 loss is 0.3390233516693115
At batch 3100 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 3100 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 3100 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.13205331563949585
Batch 3200 loss is 0.4540155231952667
At batch 3200 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 3200 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 3200 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.132124125957489
Batch 3300 loss is 0.4517333507537842
At batch 3300 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 3300 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 3300 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.11386673152446747
Batch 3400 loss is 0.4771827161312103
At batch 3400 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 3400 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 3400 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.13492326438426971
Batch 3500 loss is 0.570072591304779
At batch 3500 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 3500 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 3500 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.12627960741519928
Batch 3600 loss is 0.45578956604003906
At batch 3600 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 3600 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 3600 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.1463165581226349
Batch 3700 loss is 0.6335014700889587
At batch 3700 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 3700 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 3700 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.14399610459804535
Batch 3800 loss is 0.5209859609603882
At batch 3800 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 3800 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 3800 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.16422419250011444
Batch 3900 loss is 0.4346863925457001
At batch 3900 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 3900 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 3900 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.15166082978248596
Batch 4000 loss is 0.5
At batch 4000 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 4000 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 4000 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.14304938912391663
Batch 4100 loss is 0.5437030792236328
At batch 4100 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 4100 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 4100 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.15037545561790466
Batch 4200 loss is 0.6489743590354919
At batch 4200 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 4200 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 4200 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.15948715806007385
Batch 4300 loss is 0.4586508274078369
At batch 4300 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 4300 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 4300 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.16920660436153412
Batch 4400 loss is 0.62763512134552
At batch 4400 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 4400 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 4400 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.1596396565437317
Batch 4500 loss is 0.4787883162498474
At batch 4500 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 4500 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 4500 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.1606132984161377
Batch 4600 loss is 0.47969767451286316
At batch 4600 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 4600 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 4600 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.175163134932518
Batch 4700 loss is 0.5627346038818359
At batch 4700 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 4700 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 4700 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.16541539132595062
Batch 4800 loss is 0.5834729671478271
At batch 4800 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 4800 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 4800 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.16610798239707947
Batch 4900 loss is 0.5421139001846313
At batch 4900 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 4900 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 4900 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.16308876872062683
Batch 5000 loss is 0.4338788092136383
At batch 5000 layer 0 has weights [0.5005586743354797, 0.18302100896835327] and bias -0.5391217470169067
At batch 5000 layer 3 has weights [0.1470509171485901, -0.25655537843704224] and bias -0.22481068968772888
At batch 5000 layer 5 has weights [-0.1721118688583374, 0.4200904965400696] and bias 0.14731629192829132
