Batch 0 loss is 0.7695671319961548
At batch 0 layer 0 has weights [0.16968053579330444, 0.2158595323562622] and bias -0.4431353211402893
Batch 100 loss is 0.5146733522415161
At batch 100 layer 0 has weights [0.26578831672668457, 0.3119671940803528] and bias -0.34313660860061646
Batch 200 loss is 0.32713446021080017
At batch 200 layer 0 has weights [0.36491498351097107, 0.4110938608646393] and bias -0.24313779175281525
Batch 300 loss is 0.1574605107307434
At batch 300 layer 0 has weights [0.46163275837898254, 0.5078116059303284] and bias -0.143137589097023
Batch 400 loss is 0.027476564049720764
At batch 400 layer 0 has weights [0.5122690200805664, 0.5584480166435242] and bias -0.06688658148050308
Batch 500 loss is 0.013749511912465096
At batch 500 layer 0 has weights [0.4898838400840759, 0.5360627174377441] and bias -0.025392957031726837
Batch 600 loss is 9.086982754524797e-05
At batch 600 layer 0 has weights [0.4768275320529938, 0.52300626039505] and bias 0.0001246187894139439
Batch 700 loss is 0.0002571594959590584
At batch 700 layer 0 has weights [0.47674447298049927, 0.5229231715202332] and bias -8.060715481406078e-05
Batch 800 loss is 0.0006764824502170086
At batch 800 layer 0 has weights [0.47716420888900757, 0.5233428478240967] and bias 0.000359534053131938
Batch 900 loss is 2.2236687073018402e-05
At batch 900 layer 0 has weights [0.47693246603012085, 0.5231114029884338] and bias -1.9713088477146812e-05
Batch 1000 loss is 0.00041710061486810446
At batch 1000 layer 0 has weights [0.47643646597862244, 0.5226153135299683] and bias -2.26892461796524e-06
Batch 1100 loss is 0.00011347760300850496
At batch 1100 layer 0 has weights [0.4768902659416199, 0.5230692625045776] and bias 0.00013494818995241076
Batch 1200 loss is 8.05027739261277e-05
At batch 1200 layer 0 has weights [0.4768325984477997, 0.5230117440223694] and bias 9.921391028910875e-05
Batch 1300 loss is 0.00020781351486220956
At batch 1300 layer 0 has weights [0.4771634340286255, 0.5233426690101624] and bias 3.3801588870119303e-05
Batch 1400 loss is 3.320183532196097e-05
At batch 1400 layer 0 has weights [0.4769066274166107, 0.5230856537818909] and bias 3.7572641303995624e-05
Batch 1500 loss is 0.00011020660167559981
At batch 1500 layer 0 has weights [0.476882666349411, 0.5230617523193359] and bias -8.06837051641196e-05
Batch 1600 loss is 0.00010431979899294674
At batch 1600 layer 0 has weights [0.4770110845565796, 0.523189902305603] and bias -7.000252662692219e-05
Batch 1700 loss is 0.00024137318541761488
At batch 1700 layer 0 has weights [0.4767846465110779, 0.522963285446167] and bias -0.00010745623876573518
Batch 1800 loss is 0.0003864314057864249
At batch 1800 layer 0 has weights [0.47675034403800964, 0.5229293704032898] and bias -0.00025630558957345784
Batch 1900 loss is 0.0002385401021456346
At batch 1900 layer 0 has weights [0.47705045342445374, 0.5232290029525757] and bias 9.882912127068266e-05
Batch 2000 loss is 0.0001370853860862553
At batch 2000 layer 0 has weights [0.4768490195274353, 0.5230277180671692] and bias -9.855670214165002e-05
Batch 2100 loss is 0.0002756848407443613
At batch 2100 layer 0 has weights [0.4765796363353729, 0.5227584838867188] and bias 0.00010996809578500688
Batch 2200 loss is 2.1832802303833887e-05
At batch 2200 layer 0 has weights [0.4769294559955597, 0.5231083631515503] and bias -2.8273596399230883e-05
Batch 2300 loss is 0.00027760618831962347
At batch 2300 layer 0 has weights [0.47705602645874023, 0.523235023021698] and bias 0.00011387333506718278
Batch 2400 loss is 0.0001293457899009809
At batch 2400 layer 0 has weights [0.47692015767097473, 0.5230986475944519] and bias 0.00011935229849768803
Batch 2500 loss is 0.0002532883663661778
At batch 2500 layer 0 has weights [0.47666892409324646, 0.5228475332260132] and bias 0.00018032771185971797
Batch 2600 loss is 0.00014002130774315447
At batch 2600 layer 0 has weights [0.47683656215667725, 0.5230153203010559] and bias -7.521896623075008e-05
Batch 2700 loss is 0.00027151519316248596
At batch 2700 layer 0 has weights [0.47717127203941345, 0.5233498811721802] and bias -8.583312592236325e-05
Batch 2800 loss is 0.0001126565839513205
At batch 2800 layer 0 has weights [0.4769960343837738, 0.5231743454933167] and bias 3.282478792243637e-05
Batch 2900 loss is 0.00021143886260688305
At batch 2900 layer 0 has weights [0.47669386863708496, 0.5228721499443054] and bias 0.0003056302375625819
Batch 3000 loss is 0.0003888168721459806
At batch 3000 layer 0 has weights [0.4768306016921997, 0.5230088829994202] and bias -0.0003135711594950408
Batch 3100 loss is 0.00027848774334415793
At batch 3100 layer 0 has weights [0.4767601490020752, 0.5229383111000061] and bias -0.00017484018462710083
Batch 3200 loss is 0.0008096790406852961
At batch 3200 layer 0 has weights [0.47656798362731934, 0.5227460861206055] and bias -0.00046672343160025775
Batch 3300 loss is 0.00033656947198323905
At batch 3300 layer 0 has weights [0.47695913910865784, 0.5231368541717529] and bias 0.0003065693308599293
Batch 3400 loss is 0.00012077584688086063
At batch 3400 layer 0 has weights [0.4769543409347534, 0.5231319665908813] and bias -0.00016930715355556458
Batch 3500 loss is 0.0007011740235611796
At batch 3500 layer 0 has weights [0.47713372111320496, 0.5233112573623657] and bias 0.00045086629688739777
Batch 3600 loss is 0.00017435986956115812
At batch 3600 layer 0 has weights [0.47673752903938293, 0.5229151248931885] and bias 0.00016302961739711463
Batch 3700 loss is 0.0003458247520029545
At batch 3700 layer 0 has weights [0.47656089067459106, 0.5227384567260742] and bias 0.0003142994246445596
Batch 3800 loss is 0.00023234255786519498
At batch 3800 layer 0 has weights [0.47708913683891296, 0.5232664346694946] and bias 7.676402310607955e-05
Batch 3900 loss is 0.0001720717118587345
At batch 3900 layer 0 has weights [0.47697561979293823, 0.5231523513793945] and bias 0.00011205778719158843
Batch 4000 loss is 0.00021032779477536678
At batch 4000 layer 0 has weights [0.47677722573280334, 0.522953987121582] and bias -6.750378088327125e-05
Batch 4100 loss is 0.00013787124771624804
At batch 4100 layer 0 has weights [0.4767846465110779, 0.522961437702179] and bias 6.879780266899616e-05
Batch 4200 loss is 0.00012808505562134087
At batch 4200 layer 0 has weights [0.47678142786026, 0.5229583382606506] and bias 0.00011394138709874824
Batch 4300 loss is 0.0006500797462649643
At batch 4300 layer 0 has weights [0.47725221514701843, 0.523429274559021] and bias 0.0003092919068876654
Batch 4400 loss is 0.0001605036377441138
At batch 4400 layer 0 has weights [0.4769737720489502, 0.5231508612632751] and bias -0.0002345339162275195
Batch 4500 loss is 0.000244422466494143
At batch 4500 layer 0 has weights [0.4770243465900421, 0.5232014656066895] and bias 0.00011741817434085533
Batch 4600 loss is 0.00011117410758743063
At batch 4600 layer 0 has weights [0.47687751054763794, 0.5230547189712524] and bias 0.00014719225873705
Batch 4700 loss is 0.00024022303114179522
At batch 4700 layer 0 has weights [0.4769357740879059, 0.5231126546859741] and bias 0.00021603987261187285
Batch 4800 loss is 0.00027302937814965844
At batch 4800 layer 0 has weights [0.4767606258392334, 0.5229375958442688] and bias -0.00011270656250417233
Batch 4900 loss is 0.00023447745479643345
At batch 4900 layer 0 has weights [0.47686198353767395, 0.5230387449264526] and bias -0.00018481958250049502
Batch 5000 loss is 0.00019838004664052278
At batch 5000 layer 0 has weights [0.47666001319885254, 0.5228365659713745] and bias -3.869593001581961e-06
