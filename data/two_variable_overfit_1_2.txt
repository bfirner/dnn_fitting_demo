Batch 0 loss is 0.4052249789237976
At batch 0 layer 0 has weights [-0.3914049565792084, 0.5766748785972595] and bias 0.4005168080329895
Batch 100 loss is 0.29003122448921204
At batch 100 layer 0 has weights [-0.29204636812210083, 0.6760335564613342] and bias 0.40387409925460815
Batch 200 loss is 0.22201521694660187
At batch 200 layer 0 has weights [-0.19358959794044495, 0.774490237236023] and bias 0.4089984893798828
Batch 300 loss is 0.18220724165439606
At batch 300 layer 0 has weights [-0.14584513008594513, 0.8222349882125854] and bias 0.3237866759300232
Batch 400 loss is 0.12272653728723526
At batch 400 layer 0 has weights [-0.10676682740449905, 0.8613137602806091] and bias 0.243740513920784
Batch 500 loss is 0.09766708314418793
At batch 500 layer 0 has weights [-0.0708695650100708, 0.8972106575965881] and bias 0.1735304743051529
Batch 600 loss is 0.03669769689440727
At batch 600 layer 0 has weights [-0.03704849258065224, 0.9310316443443298] and bias 0.10627127438783646
Batch 700 loss is 0.02635878697037697
At batch 700 layer 0 has weights [-0.004168449901044369, 0.9639120697975159] and bias 0.040066082030534744
Batch 800 loss is 0.00015045792679302394
At batch 800 layer 0 has weights [0.015917690470814705, 0.9839982986450195] and bias 0.00020556279923766851
Batch 900 loss is 0.0002817272616084665
At batch 900 layer 0 has weights [0.015853120014071465, 0.9839334487915039] and bias -0.00017501106776762754
Batch 1000 loss is 0.00030321136000566185
At batch 1000 layer 0 has weights [0.015629680827260017, 0.9837096333503723] and bias 0.00011341892241034657
Batch 1100 loss is 9.163844515569508e-05
At batch 1100 layer 0 has weights [0.015873979777097702, 0.9839540719985962] and bias -3.251863381592557e-05
Batch 1200 loss is 0.00013442868657875806
At batch 1200 layer 0 has weights [0.015812233090400696, 0.9838925004005432] and bias 4.186513979220763e-05
Batch 1300 loss is 5.921520642004907e-05
At batch 1300 layer 0 has weights [0.016018979251384735, 0.9840993881225586] and bias -5.656078792526387e-05
Batch 1400 loss is 0.0002821703674271703
At batch 1400 layer 0 has weights [0.015822263434529305, 0.983902633190155] and bias -0.0001704286551102996
Batch 1500 loss is 0.00017811375437304378
At batch 1500 layer 0 has weights [0.015872037038207054, 0.9839522838592529] and bias -0.00010126198321813717
Batch 1600 loss is 0.00023209302162285894
At batch 1600 layer 0 has weights [0.016191741451621056, 0.9842724800109863] and bias -0.0004467814869713038
Batch 1700 loss is 0.0003557414165697992
At batch 1700 layer 0 has weights [0.016155557706952095, 0.984235942363739] and bias 0.000172212632605806
Batch 1800 loss is 6.089425369282253e-05
At batch 1800 layer 0 has weights [0.016022106632590294, 0.9841026663780212] and bias -3.815521995420568e-05
Batch 1900 loss is 0.0001743177999742329
At batch 1900 layer 0 has weights [0.015880249440670013, 0.983960747718811] and bias -7.493378507206216e-05
Batch 2000 loss is 6.865627074148506e-05
At batch 2000 layer 0 has weights [0.015902936458587646, 0.9839834570884705] and bias 0.0001325588527834043
Batch 2100 loss is 0.00044364677160046995
At batch 2100 layer 0 has weights [0.01612146757543087, 0.984201967716217] and bias 0.00029203135636635125
Batch 2200 loss is 0.00036982266465201974
At batch 2200 layer 0 has weights [0.01631476916372776, 0.9843953847885132] and bias -0.0004023115325253457
Batch 2300 loss is 0.0006741195684298873
At batch 2300 layer 0 has weights [0.015514134429395199, 0.983594536781311] and bias -0.000256318278843537
Batch 2400 loss is 0.00024028125335462391
At batch 2400 layer 0 has weights [0.01609349437057972, 0.9841738343238831] and bias 8.986683678813279e-05
Batch 2500 loss is 0.00016823812620714307
At batch 2500 layer 0 has weights [0.016025502234697342, 0.9841057658195496] and bias 0.00010258760448778048
Batch 2600 loss is 0.00040366780012845993
At batch 2600 layer 0 has weights [0.015592117793858051, 0.9836724996566772] and bias -0.00010491516877664253
Batch 2700 loss is 0.00015014034579508007
At batch 2700 layer 0 has weights [0.015978334471583366, 0.984058678150177] and bias 0.0001397599116899073
Batch 2800 loss is 0.0002080620324704796
At batch 2800 layer 0 has weights [0.01581895723938942, 0.9838991761207581] and bias -8.473281923215836e-05
Batch 2900 loss is 2.8601294616237283e-05
At batch 2900 layer 0 has weights [0.015961861237883568, 0.9840419888496399] and bias 2.7045462047681212e-05
Batch 3000 loss is 0.00013926022802479565
At batch 3000 layer 0 has weights [0.016140475869178772, 0.9842206239700317] and bias -4.848061871598475e-05
Batch 3100 loss is 0.0004839659377466887
At batch 3100 layer 0 has weights [0.015687163919210434, 0.9837672710418701] and bias -0.00022824096959084272
Batch 3200 loss is 0.00027781256358139217
At batch 3200 layer 0 has weights [0.016047392040491104, 0.9841274619102478] and bias 0.0001958169596036896
Batch 3300 loss is 5.514343501999974e-05
At batch 3300 layer 0 has weights [0.015948588028550148, 0.9840286374092102] and bias 6.440722791012377e-05
Batch 3400 loss is 0.00039368998841382563
At batch 3400 layer 0 has weights [0.015893159434199333, 0.9839736819267273] and bias -0.00030216400045901537
Batch 3500 loss is 2.4387394660152495e-05
At batch 3500 layer 0 has weights [0.01593925803899765, 0.9840197563171387] and bias -9.01485236681765e-06
Batch 3600 loss is 0.00019587120914366096
At batch 3600 layer 0 has weights [0.016145862638950348, 0.9842265248298645] and bias -0.00022484528017230332
Batch 3700 loss is 0.0002161226875614375
At batch 3700 layer 0 has weights [0.01596657931804657, 0.9840472340583801] and bias -0.0002234815910924226
Batch 3800 loss is 7.396961154881865e-05
At batch 3800 layer 0 has weights [0.015882616862654686, 0.9839633703231812] and bias 2.8319689590716735e-05
Batch 3900 loss is 2.602830863907002e-05
At batch 3900 layer 0 has weights [0.015969298779964447, 0.984049916267395] and bias 1.7018326616380364e-05
Batch 4000 loss is 0.00038794497959315777
At batch 4000 layer 0 has weights [0.016127455979585648, 0.9842081665992737] and bias 0.00018868256302084774
Batch 4100 loss is 0.00036833243211731315
At batch 4100 layer 0 has weights [0.015591158531606197, 0.9836722016334534] and bias 0.0004392973496578634
Batch 4200 loss is 0.00013065811072010547
At batch 4200 layer 0 has weights [0.01582011766731739, 0.9839015007019043] and bias 0.00020729872630909085
Batch 4300 loss is 0.00040468823863193393
At batch 4300 layer 0 has weights [0.015761563554406166, 0.9838430285453796] and bias -0.00021932876552455127
Batch 4400 loss is 0.00018517914577387273
At batch 4400 layer 0 has weights [0.016095425933599472, 0.9841770529747009] and bias 6.598000618396327e-05
Batch 4500 loss is 2.1808336896356195e-05
At batch 4500 layer 0 has weights [0.01597236841917038, 0.9840540885925293] and bias 7.74964428273961e-06
Batch 4600 loss is 0.00016986271657515317
At batch 4600 layer 0 has weights [0.016117148101329803, 0.9841985702514648] and bias -6.179446791065857e-05
Batch 4700 loss is 0.0004300281289033592
At batch 4700 layer 0 has weights [0.016048796474933624, 0.9841302633285522] and bias -0.0005083600408397615
Batch 4800 loss is 0.00015817601524759084
At batch 4800 layer 0 has weights [0.01602863147854805, 0.9841101169586182] and bias 8.878148946678266e-05
Batch 4900 loss is 5.95081910432782e-05
At batch 4900 layer 0 has weights [0.015904996544122696, 0.983986496925354] and bias -8.671486284583807e-06
Batch 5000 loss is 0.0004839724279008806
At batch 5000 layer 0 has weights [0.016164420172572136, 0.9842458367347717] and bias 0.0003623231314122677
