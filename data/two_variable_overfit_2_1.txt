Batch 0 loss is 0.8133686780929565
At batch 0 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 0 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias 0.12901759147644043
At batch 0 layer 5 has weights [-0.46134763956069946, -0.1278613805770874] and bias -0.19134670495986938
Batch 100 loss is 0.7286956310272217
At batch 100 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 100 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias 0.03248796612024307
At batch 100 layer 5 has weights [-0.38011300563812256, -0.1278613805770874] and bias -0.09134650230407715
Batch 200 loss is 0.5
At batch 200 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 200 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009132503531873226
At batch 200 layer 5 has weights [-0.3677903413772583, -0.1278613805770874] and bias 0.006173253059387207
Batch 300 loss is 0.5922374129295349
At batch 300 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 300 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 300 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.008067218586802483
Batch 400 loss is 0.4085029363632202
At batch 400 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 400 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 400 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.012015633285045624
Batch 500 loss is 0.469521164894104
At batch 500 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 500 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 500 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.012338930740952492
Batch 600 loss is 0.5306662917137146
At batch 600 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 600 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 600 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.009339209645986557
Batch 700 loss is 0.37649768590927124
At batch 700 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 700 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 700 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.005990656558424234
Batch 800 loss is 0.46885445713996887
At batch 800 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 800 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 800 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.0016711020143702626
Batch 900 loss is 0.3132387399673462
At batch 900 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 900 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 900 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.001969982637092471
Batch 1000 loss is 0.4689834415912628
At batch 1000 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 1000 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 1000 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.003735164413228631
Batch 1100 loss is 0.43761324882507324
At batch 1100 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 1100 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 1100 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.000905896071344614
Batch 1200 loss is 0.3443477153778076
At batch 1200 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 1200 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 1200 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.0019126536790281534
Batch 1300 loss is 0.4076768159866333
At batch 1300 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 1300 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 1300 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.007609588094055653
Batch 1400 loss is 0.5306705832481384
At batch 1400 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 1400 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 1400 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.009270443581044674
Batch 1500 loss is 0.40860873460769653
At batch 1500 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 1500 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 1500 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.012579863891005516
Batch 1600 loss is 0.5302289128303528
At batch 1600 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 1600 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 1600 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.016337690874934196
Batch 1700 loss is 0.5
At batch 1700 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 1700 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 1700 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.012503983452916145
Batch 1800 loss is 0.5
At batch 1800 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 1800 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 1800 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.012401046231389046
Batch 1900 loss is 0.5
At batch 1900 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 1900 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 1900 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.01648031733930111
Batch 2000 loss is 0.5593417882919312
At batch 2000 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 2000 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 2000 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.02526530623435974
Batch 2100 loss is 0.443504273891449
At batch 2100 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 2100 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 2100 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.0480341799557209
Batch 2200 loss is 0.5856348276138306
At batch 2200 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 2200 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 2200 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.043280936777591705
Batch 2300 loss is 0.44246214628219604
At batch 2300 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 2300 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 2300 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.03969711810350418
Batch 2400 loss is 0.4121812582015991
At batch 2400 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 2400 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 2400 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.03163335472345352
Batch 2500 loss is 0.47009533643722534
At batch 2500 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 2500 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 2500 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.021525025367736816
Batch 2600 loss is 0.5300917029380798
At batch 2600 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 2600 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 2600 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.018532615154981613
Batch 2700 loss is 0.5
At batch 2700 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 2700 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 2700 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.023790430277585983
Batch 2800 loss is 0.5599230527877808
At batch 2800 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 2800 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 2800 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.020615365356206894
Batch 2900 loss is 0.470064252614975
At batch 2900 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 2900 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 2900 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.021027972921729088
Batch 3000 loss is 0.680018424987793
At batch 3000 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 3000 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 3000 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.019950976595282555
Batch 3100 loss is 0.5602483749389648
At batch 3100 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 3100 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 3100 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.018013298511505127
Batch 3200 loss is 0.47024157643318176
At batch 3200 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 3200 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 3200 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.023865489289164543
Batch 3300 loss is 0.6180413961410522
At batch 3300 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 3300 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 3300 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.027834473177790642
Batch 3400 loss is 0.5
At batch 3400 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 3400 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 3400 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.025407789275050163
Batch 3500 loss is 0.3804771304130554
At batch 3500 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 3500 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 3500 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.0219084694981575
Batch 3600 loss is 0.5611556768417358
At batch 3600 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 3600 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 3600 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.010754898190498352
Batch 3700 loss is 0.5292780995368958
At batch 3700 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 3700 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 3700 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.03155011683702469
Batch 3800 loss is 0.5291657447814941
At batch 3800 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 3800 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 3800 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.03334773704409599
Batch 3900 loss is 0.6392478942871094
At batch 3900 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 3900 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 3900 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.05440684035420418
Batch 4000 loss is 0.5
At batch 4000 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 4000 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 4000 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.07095388323068619
Batch 4100 loss is 0.5
At batch 4100 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 4100 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 4100 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.04735500365495682
Batch 4200 loss is 0.5571169853210449
At batch 4200 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 4200 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 4200 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.043063949793577194
Batch 4300 loss is 0.5
At batch 4300 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 4300 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 4300 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.04531444236636162
Batch 4400 loss is 0.6982431411743164
At batch 4400 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 4400 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 4400 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.046872880309820175
Batch 4500 loss is 0.528325617313385
At batch 4500 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 4500 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 4500 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.04679001867771149
Batch 4600 loss is 0.5279843807220459
At batch 4600 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 4600 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 4600 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.05224985256791115
Batch 4700 loss is 0.36290687322616577
At batch 4700 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 4700 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 4700 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.061301976442337036
Batch 4800 loss is 0.5542554259300232
At batch 4800 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 4800 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 4800 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.06595668941736221
Batch 4900 loss is 0.4198050796985626
At batch 4900 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 4900 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 4900 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.072293721139431
Batch 5000 loss is 0.4457012414932251
At batch 5000 layer 0 has weights [-0.20647215843200684, -0.2558818459510803] and bias -0.36369600892066956
At batch 5000 layer 3 has weights [-0.2642374634742737, 0.5264982581138611] and bias -0.009144802577793598
At batch 5000 layer 5 has weights [-0.3677888512611389, -0.1278613805770874] and bias 0.06577523797750473
